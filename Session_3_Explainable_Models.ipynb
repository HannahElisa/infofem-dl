{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Session 3\n",
    "# Explainable Convolutional Neural Networks\n",
    "\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Boilerplate code\n",
    "\n",
    "This is the same as in the previous session; all cells are merged and printouts of versions/GPU are omitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Input, InputLayer, Conv2D, MaxPool2D, Flatten, Dense, UpSampling2D, LocallyConnected2D, SpatialDropout2D, BatchNormalization\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "!test -e tmp_slices.npz || curl -L \"https://drive.google.com/uc?export=download&id=1R2-H0dhhrj6XNK7Q-MazIWGeFDOf6Zya\" --output tmp_slices.npz\n",
    "\n",
    "TRAINING_SLICE_COUNT = 300\n",
    "\n",
    "loaded = np.load('tmp_slices.npz')\n",
    "\n",
    "x_train = loaded['x_train'][:TRAINING_SLICE_COUNT]\n",
    "y_train = loaded['y_train'][:TRAINING_SLICE_COUNT]\n",
    "\n",
    "x_test = loaded['x_train'][TRAINING_SLICE_COUNT:]\n",
    "y_test = loaded['y_train'][TRAINING_SLICE_COUNT:]\n",
    "\n",
    "assert len(x_train) == len(y_train)\n",
    "\n",
    "example_test_slice = 1800\n",
    "\n",
    "# remove the lesion labels (values 2..3)\n",
    "y_train_binary = y_train.clip(0, 1)\n",
    "y_test_binary = y_test.clip(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5. Segmentation: Auto Encoder (AE)-Style\n",
    "* We define short functions that return a model.\n",
    "* The first is a simple architecture that collapses and expands an image into the desired mask, similar to an Auto Encoder (AE).\n",
    "* The second is the famous U-Net.\n",
    "\n",
    "Further reading: Variational AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getDropoutBayesModel(_filters=32, filters_add=0, _kernel_size=(3,3), _padding='same', _activation='relu', _kernel_regularizer=None, _final_layer_nonlinearity='sigmoid'):\n",
    "    model = Sequential()\n",
    "    # We are indifferent about the xy size, but accept only one channel (gray value images). This has the consequence that debugging sizes gets harder.\n",
    "    input_layer = Input(shape=(None,None,1))\n",
    "    \n",
    "    x = BatchNormalization()(input_layer)\n",
    "    x = Conv2D(filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, name='firstConvolutionalLayer')(x)\n",
    "    x = SpatialDropout2D(0.3)(x, training=True)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = SpatialDropout2D(0.3)(x, training=True)\n",
    "    x = MaxPool2D()(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters+filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = SpatialDropout2D(0.3)(x, training=True)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters+filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = SpatialDropout2D(0.3)(x, training=True)\n",
    "    x = MaxPool2D()(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters+2*filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters+2*filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = UpSampling2D()(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters+filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters+filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = UpSampling2D()(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    output_layer = Conv2D(1, kernel_size=(1,1), activation=_final_layer_nonlinearity)(x)\n",
    "    \n",
    "    model = Model(input_layer, output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow_probability.python.layers import Convolution2DFlipout\n",
    "\n",
    "def getProbBayesModel(_filters=32, filters_add=0, _kernel_size=(3,3), _padding='same', _activation='relu', _kernel_regularizer=None, _final_layer_nonlinearity='sigmoid'):\n",
    "    model = Sequential()\n",
    "    # We are indifferent about the xy size, but accept only one channel (gray value images). This has the consequence that debugging sizes gets harder.\n",
    "    input_layer = Input(shape=(None,None,1))\n",
    "    \n",
    "    x = BatchNormalization()(input_layer)\n",
    "    x = Conv2D(filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, name='firstConvolutionalLayer')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = MaxPool2D()(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters+filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters+filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = MaxPool2D()(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters+2*filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters+2*filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = UpSampling2D()(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters+filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters+filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = UpSampling2D()(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    output_layer = Convolution2DFlipout(1, kernel_size=(1,1), activation=_final_layer_nonlinearity)(x)\n",
    "    \n",
    "    model = Model(input_layer, output_layer)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow_probability.python.layers import Convolution2DFlipout\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "\n",
    "def getProbBayesModernModel(_filters=32, filters_add=0, _kernel_size=(3,3), _padding='same', _activation='relu', _kernel_regularizer=None, _final_layer_nonlinearity='sigmoid'):\n",
    "    model = Sequential()\n",
    "    # We are indifferent about the xy size, but accept only one channel (gray value images). This has the consequence that debugging sizes gets harder.\n",
    "    input_layer = Input(shape=(None, None,1))\n",
    "    \n",
    "    x = BatchNormalization()(input_layer)\n",
    "    x = Conv2D(filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters, kernel_size=_kernel_size, strides=(2,2), padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters+filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters+filters_add, kernel_size=_kernel_size, strides=(2,2), padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters+2*filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2DTranspose(filters=_filters+2*filters_add, kernel_size=_kernel_size, strides=2, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters+filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2DTranspose(filters=_filters+filters_add, kernel_size=_kernel_size, strides=2, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    output_layer = Convolution2DFlipout(1, kernel_size=(1,1), activation=_final_layer_nonlinearity)(x)\n",
    "    \n",
    "    model = Model(input_layer, output_layer)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def pad_image_for_model(model, input_image):\n",
    "    '''Determine the necessary amount of padding\n",
    "    (difference between input and output size of the model)\n",
    "    and apply it to an ndarry with one or more images.'''\n",
    "    \n",
    "    padding = 0\n",
    "    if 'firstConvolutionalLayer' in [layer.name for layer in model.layers]:\n",
    "        if model.get_layer('firstConvolutionalLayer').padding == 'valid':\n",
    "            padding = 20 # WARNING: Hard-coded for above architecture!\n",
    "\n",
    "            # determine in which dimension to apply this padding\n",
    "            ndim_padding = []\n",
    "            if np.ndim(input_image) > 2:\n",
    "                # do not pad along batch dimension (if present)\n",
    "                ndim_padding.append((0, 0))\n",
    "            ndim_padding.append((padding, padding)) # pad above/below image (y dimension)\n",
    "            ndim_padding.append((padding, padding)) # pad left/right of image (x dimension)\n",
    "            if np.ndim(input_image) > 3:\n",
    "                # do not pad along channel dimension (if present)\n",
    "                ndim_padding.append((0, 0))\n",
    "\n",
    "            input_image = np.lib.pad(input_image, ndim_padding,\n",
    "                                     #'constant', constant_values = 0)\n",
    "                                     'reflect')\n",
    "\n",
    "    return input_image, padding\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class VisualHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        # also show initial prediction\n",
    "        plot_prediction(self.model, example_test_slice)\n",
    "    \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        # show prediction after every training epoch\n",
    "        plot_prediction(self.model, example_test_slice)\n",
    "        \n",
    "vh_callback = VisualHistory()\n",
    "\n",
    "def do_prediction(model, input_image, verbose = False):\n",
    "    # first do padding of full slice\n",
    "    input_image, padding = pad_image_for_model(model, input_image)\n",
    "    \n",
    "    # add batch and channel dimensions (network expects 4D arrays)\n",
    "    input_array = input_image[np.newaxis,:,:,np.newaxis]\n",
    "    if verbose:\n",
    "        print(\"input shape:\", input_array.shape)\n",
    "\n",
    "    y_predicted = model.predict(input_array)\n",
    "    if verbose:\n",
    "        print(\"output shape:\", y_predicted.shape)\n",
    "\n",
    "    return input_image, input_array, y_predicted, padding\n",
    "\n",
    "def plot_prediction(model, pred_slice_index):\n",
    "    # get single slice\n",
    "    input_image    = x_test[pred_slice_index]\n",
    "    # could use y_train_binary here for the first half of the notebook, but in the end we want to see the lesion\n",
    "    reference_mask = y_test[pred_slice_index]\n",
    "\n",
    "    input_image, input_array, y_predicted, padding = do_prediction(model, input_image)\n",
    "    \n",
    "    padded_extent = np.array([0, input_array.shape[2], input_array.shape[1], 0]) - 0.5 - padding\n",
    "\n",
    "    # display prediction for inspection\n",
    "    f, ax = plt.subplots(1, 5 if padding else 4, figsize = (11 if padding else 8, 3), sharey = True)\n",
    "    ax[0].imshow(x_test[pred_slice_index])\n",
    "    ax[0].set_title('orig')\n",
    "    if padding:\n",
    "        ax[1].imshow(input_array[0,:,:,0], extent = padded_extent)\n",
    "        ax[1].set_title('padded input')\n",
    "    ax[-2].imshow(y_predicted[0,:,:,0])\n",
    "    ax[-2].set_title('predicted mask')\n",
    "    ax[-3].imshow(reference_mask.clip(0,1))\n",
    "    ax[-3].set_title('reference mask')\n",
    "    ax[-1].imshow(reference_mask.clip(0,1) - y_predicted[0,:,:,0])\n",
    "    ax[-1].set_title('(ref - predicted)')\n",
    "    ax[0].set_ylim(*padded_extent[2:])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "modelBayesValid = getDropoutBayesModel(_padding='valid')\n",
    "modelBayesValid.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "print(\"Model parameters: {0:,}\".format(modelBayesValid.count_params()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ProbBayes = getProbBayesModel(_padding='valid')\n",
    "model_ProbBayes.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "print(\"Model parameters: {0:,}\".format(model_ProbBayes.count_params()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ProbBayesModern = getProbBayesModernModel(_padding='same')\n",
    "model_ProbBayesModern.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "print(\"Model parameters: {0:,}\".format(model_ProbBayesModern.count_params()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for _padding = 'valid' and 'reflect' padding\n",
    "historyBayesValid = modelBayesValid.fit(np.lib.pad(x_train[...,np.newaxis],\n",
    "                                         [(0,0), (20,20), (20,20), (0,0)], 'reflect'),\n",
    "                              y_train_binary[...,np.newaxis],\n",
    "                              batch_size=20, epochs=5, callbacks=[vh_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic Conv2D layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for _padding = 'valid' and 'reflect' padding\n",
    "historyProbBayesValid = model_ProbBayes.fit(np.lib.pad(x_train[...,np.newaxis],\n",
    "                                         [(0,0), (20,20), (20,20), (0,0)], 'reflect'),\n",
    "                              y_train_binary[...,np.newaxis],\n",
    "                              batch_size=20, epochs=5, callbacks=[vh_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for _padding = 'same'.\n",
    "# Model needs no padding even for \"valid\"\n",
    "historyProbBayesModernValid = model_ProbBayesModern.fit(x_train[...,np.newaxis],\n",
    "                              y_train_binary[...,np.newaxis],\n",
    "                              batch_size=20, epochs=5, callbacks=[vh_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction using Bayes by Dropout on the initial model\n",
    "\n",
    "For this approach, the prediction needs to run $n$ times, averaging the results per voxel for the final prediction.\n",
    "\n",
    "Warning: this unconditionally puts all layers into training mode, also the BatchNormalization, which will lead to side effects. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prediction function from the model, setting the learning phase to \"learn\" to let dropout be active.\n",
    "f = K.function([modelValid.layers[0].input, K.learning_phase()],\n",
    "               [modelValid.layers[-1].output])\n",
    "\n",
    "# This takes some memory. In my example, all 566 test images won't be processed on a 4GB GPU.\n",
    "def predict_with_uncertainty(f, x, no_classes, n_iter=20):\n",
    "    result = np.zeros( (n_iter,) + (x.shape) + (no_classes,) )\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        result[i,:, :] = f((x, 1))[0]\n",
    "    prediction = result.mean(axis=0)\n",
    "    uncertainty = result.std(axis=0)\n",
    "    return prediction, uncertainty, result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test0 = x_test[0:2][...,np.newaxis]\n",
    "\n",
    "print(x_test0.shape, len(x_test0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test0 = pad_image_for_model(modelValid, x_test0)\n",
    "result = np.zeros((20,) + (len(x_test0),) + (x_test[0].shape) + (1,) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(f((x_test0,1))),f((x_test0,1))[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    result[i,:,:,:] = f((x_test0,1))[0]\n",
    "    \n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred0 = do_prediction(modelValid, x_test[0])\n",
    "y_pred1 = do_prediction(modelValid, x_test[1])\n",
    "y_pred0[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = result.mean(axis=0)\n",
    "uncertainty = result.std(axis=0)\n",
    "print(prediction.shape, uncertainty.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(in_image, in_mask, direct_prediction, prob_prediction, uncertainty):\n",
    "    f, ax = plt.subplots(1, 5, figsize = (22, 6), sharey = True)\n",
    "    ax[0].imshow(in_image)\n",
    "    ax[0].set_title('orig')\n",
    "    ax[1].imshow(in_mask)\n",
    "    ax[1].set_title('mask')\n",
    "    ax[2].imshow(direct_prediction)\n",
    "    ax[2].set_title('mask')\n",
    "    ax[3].imshow(prob_prediction)\n",
    "    ax[3].set_title('prob. prediction')\n",
    "    ax[4].imshow(uncertainty)\n",
    "    ax[4].set_title('uncertainty')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(x_test[0], y_test[0], y_pred0[2][0,:,:,0], prediction[0,:,:,0],uncertainty[0,:,:,0])\n",
    "plot_result(x_test[1], y_test[1], y_pred1[2][0,:,:,0], prediction[1,:,:,0],uncertainty[1,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prediction\n",
    "Let's look at the prediction from some more example slices, but let's only use the `x_test` slices that we did not use for training. (In a real scenario, we would to the separation of training & test data on the level of patients, *before* extracting slices, and we'd also have a validation set.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_indices = np.random.choice(x_test.shape[0], 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model with 'valid' convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in slice_indices:\n",
    "    plot_prediction(modelValid, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model with 'same' convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for a in slice_indices:\n",
    "    plot_prediction(modelSame, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tile-based Prediction\n",
    "\n",
    "Let's predict the image divided into upper and lower half, and then in full. Note that the network could also predict smaller or larger tile sizes.  Such an approach is necessary for large images, or with volumetric data and 3D convolutions, when it is not possible to have the whole image in (GPU) memory at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hand-picked slice useful to visualize benefits of tiled approach\n",
    "example_test_slice = 505 - TRAINING_SLICE_COUNT\n",
    "assert example_test_slice > 0, 'when increasing TRAINING_SLICE_COUNT, please pick a new example slice as well'\n",
    "\n",
    "\n",
    "def plot_prediction_tiled(model, pred_slice_index, tile = None):\n",
    "    # get single slice\n",
    "    input_image    = x_train[pred_slice_index]\n",
    "    reference_mask = y_train_binary[pred_slice_index]\n",
    "\n",
    "    # first do padding of full slice\n",
    "    input_image, padding = pad_image_for_model(model, input_image)\n",
    "    \n",
    "    # add batch and channel dimensions (network expects 4D arrays)\n",
    "    input_array = input_image[np.newaxis,:,:,np.newaxis]\n",
    "    print(\"padded input shape:\", input_array.shape)\n",
    "\n",
    "    # then cut the specified tile box (plus padding)\n",
    "    if tile is not None: # (default is full slice)\n",
    "        tile = np.asarray(tile)\n",
    "        assert tile.shape == (2, 2)\n",
    "        padded_tile = tile.copy()\n",
    "        padded_tile[:,1] += 2*padding\n",
    "        input_array = input_array[:,\n",
    "                                  padded_tile[0][0]:padded_tile[0][1],\n",
    "                                  padded_tile[1][0]:padded_tile[1][1],\n",
    "                                  :]\n",
    "        reference_mask = reference_mask[tile[0][0]:tile[0][1],\n",
    "                                        tile[1][0]:tile[1][1]]\n",
    "        print(\"tiled padded shape:\", input_array.shape)\n",
    "\n",
    "    y_predicted = model.predict(input_array)\n",
    "    print(\"output shape:\", y_predicted.shape)\n",
    "\n",
    "    padded_extent = np.array([0,input_array.shape[2],input_array.shape[1],0]) - 0.5 - padding\n",
    "    orig_extent = np.array([0,reference_mask.shape[1],reference_mask.shape[0],0]) - 0.5\n",
    "    if tile is not None:\n",
    "        padded_extent[:2] += tile[1,0]\n",
    "        padded_extent[2:] += tile[0,0]\n",
    "        orig_extent[:2] += tile[1,0]\n",
    "        orig_extent[2:] += tile[0,0]\n",
    "\n",
    "    # display prediction for inspection\n",
    "    f, ax = plt.subplots(1, 4 if padding else 3, figsize = (14 if padding else 12, 3), sharey = True)\n",
    "    ax[0].imshow(x_train[pred_slice_index])\n",
    "    ax[0].set_title('orig')\n",
    "    if padding:\n",
    "        ax[1].imshow(input_array[0,:,:,0], extent = padded_extent)\n",
    "        ax[1].set_title('padded input')\n",
    "    ax[-2].imshow(y_predicted[0,:,:,0], extent = orig_extent)\n",
    "    ax[-2].set_title('predicted mask')\n",
    "    ax[-1].imshow(reference_mask, extent = orig_extent)\n",
    "    ax[-1].set_title('reference mask')\n",
    "    ax[0].set_ylim(*padded_extent[2:])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tile = [[0,42],[0,76]]\n",
    "plot_prediction_tiled(modelSame, example_test_slice, tile)\n",
    "tile = [[42,76],[0,76]]\n",
    "plot_prediction_tiled(modelSame, example_test_slice, tile)\n",
    "plot_prediction(modelSame, example_test_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile = [[0,42],[0,76]]\n",
    "plot_prediction_tiled(modelValid, example_test_slice, tile)\n",
    "tile = [[42,76],[0,76]]\n",
    "plot_prediction_tiled(modelValid, example_test_slice, tile)\n",
    "plot_prediction(modelValid, example_test_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularisation\n",
    "* Regularisation should improve convergence. Let's try and add some Batch Normalisation first. Batch Normalisation intends to normalise the input to a (convolutional) layer, so that the values in the resulting feature maps don't get exessively large. \n",
    "    * Keras offers BatchNorm layers.\n",
    "    * Include one before each convolutional layer.\n",
    "* Another regularisation measure is to choose better activation functions. Probabilistic Rectified Linear Units (PReLU) crop negative values to a small epsilon, but route through any value greater than zero.\n",
    "    * In Keras, activation functions are selected through the `activation=[softmax|elu|selu|relu|tanh|sigmoid|hard_sigmoid|linear]` parameter to a layer, each in quotes.\n",
    "    * PReLU is one of the advanced activation functions that need to be added as a layer. It has many \"trainable\" parameters. How many? Why?\n",
    "    * How many parameters does ReLU have?\n",
    "* Lastly, L1 and L2 norm can be used as additional constraints on weights, biases, and activations.\n",
    "    * In Keras, this normalisation is again a parameter to the layer initialisation, using `kernel_regularizer=[l1(0.01)|l2(0.01)|l1_l2(0.01)]`.\n",
    "    * You have to `from keras.regularizers import l1, l2, l1_l2` to enable this functionality.\n",
    "    * You can also explore `bias_regularizer` and `activity_regularizer`.\n",
    "* Make your life easier by extracting a block: Conv -- Relu -- Batch Normalization. Then play with the options -- but carefully: When you just switch everything \"on\", chances are you will get bad results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We will use the following block to generate the regularisation block\n",
    "\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "\n",
    "def addConvBN(model, filters=32, kernel_size=(3,3), batch_norm=True, activation='prelu', padding='same', kernel_regularizer=None, name = None):\n",
    "    if batch_norm:\n",
    "        model.add(BatchNormalization())\n",
    "    if activation == 'prelu':\n",
    "        model.add(Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation='linear', kernel_regularizer=kernel_regularizer, name = name))\n",
    "        model.add(PReLU())\n",
    "    elif activation == 'lrelu':\n",
    "        model.add(Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation='linear', kernel_regularizer=kernel_regularizer, name = name))\n",
    "        model.add(LeakyReLU())\n",
    "    else:\n",
    "        model.add(Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation=activation, kernel_regularizer=kernel_regularizer, name = name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# batch norm model\n",
    "def getBNModel(_filters=32, _filters_add=0, _kernel_size=(3,3), _padding='same', _activation='prelu', _kernel_regularizer=None, _final_layer_nonlinearity='sigmoid', _num_classes=1):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # this is really ugly, but TensorFlow's batch normalization\n",
    "    # currently has a limitation that it cannot work on unknown input sizes,\n",
    "    # so we need to get the height & width of our training data:\n",
    "    h, w = x_train.shape[1:]\n",
    "    if _padding == 'valid':\n",
    "        model.add(InputLayer(input_shape = (h+40, w+40, 1)))\n",
    "    elif _padding == 'same':\n",
    "        model.add(InputLayer(input_shape = (h, w, 1)))\n",
    "\n",
    "    addConvBN(model, filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, name='firstConvolutionalLayer')\n",
    "    addConvBN(model, filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)\n",
    "    model.add(MaxPool2D())\n",
    "\n",
    "    addConvBN(model, filters=_filters+_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)\n",
    "    addConvBN(model, filters=_filters+_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)\n",
    "    model.add(MaxPool2D())\n",
    "\n",
    "    addConvBN(model, filters=_filters+2*_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)\n",
    "    addConvBN(model, filters=_filters+2*_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)\n",
    "    model.add(UpSampling2D())\n",
    "\n",
    "    addConvBN(model, filters=_filters+_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)\n",
    "    addConvBN(model, filters=_filters+_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)\n",
    "    model.add(UpSampling2D())\n",
    "\n",
    "    addConvBN(model, filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)\n",
    "    addConvBN(model, filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)\n",
    "\n",
    "    model.add(Conv2D(_num_classes, kernel_size=(1,1), activation=_final_layer_nonlinearity))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Batch Norm with PReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# This network uses the PReLU layer.\n",
    "# Note the number of parameters when executing model.summary().\n",
    "modelValidBN = getBNModel(_padding='valid')\n",
    "modelValidBN.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "modelValidBN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "padded_x_train, padding = pad_image_for_model(modelValidBN, x_train[...,np.newaxis])\n",
    "historyValidBN = modelValidBN.fit(padded_x_train,\n",
    "                                  y_train_binary[...,np.newaxis],\n",
    "                                  batch_size=10, epochs=5, callbacks=[vh_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(18, 4), dpi= 80, facecolor='w', edgecolor='k')\n",
    "ax.plot(historyValid.history['loss'], label = 'without BN')\n",
    "ax.plot(historyValidBN.history['loss'], label = 'with batch normalization')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.grid()\n",
    "ax.legend(loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could use padded convolutions again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSameBN = getBNModel(_padding='same')\n",
    "modelSameBN.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "history = modelSameBN.fit(x_train[...,np.newaxis], y_train_binary[...,np.newaxis],\n",
    "                          batch_size=10, epochs=5, callbacks=[vh_callback]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Batch Norm with ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Now compare with a model with ReLU (instead of PReLu)\n",
    "model = getBNModel(_padding='valid', _activation='relu')\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "print(\"Model parameters: {0:,}\".format(model.count_params()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "padded_x_train, padding = pad_image_for_model(model, x_train[...,np.newaxis])\n",
    "history = model.fit(padded_x_train, y_train_binary[...,np.newaxis],\n",
    "                    batch_size=10, epochs=75, callbacks=[vh_callback]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regularisation using L1/L2 norm on weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from keras.regularizers import l1, l2, l1_l2\n",
    "\n",
    "model = getModel(_padding='valid', _kernel_regularizer = l2(0.001))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "print(\"Model parameters: {0:,}\".format(model.count_params()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "padded_x_train, padding = pad_image_for_model(model, x_train[...,np.newaxis])\n",
    "history = model.fit(padded_x_train,\n",
    "                    y_train_binary[...,np.newaxis],\n",
    "                    batch_size=10, epochs=75, callbacks=[vh_callback]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 6. Loss functions\n",
    "* Loss functions take the predicted output, `y_pred`, and the expected output, `y_train`, and calculate their distance. The result is the minimization target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Loss functions: Final layer non-linearity dependency\n",
    "* We have worked with binary crossentropy. See [this blog series](http://neuralnetworksanddeeplearning.com/chap3.html) for a comment:\n",
    "    > When should we use the cross-entropy instead of the quadratic cost?<p>In fact, the cross-entropy is nearly always the better choice, provided the output neurons are sigmoid neurons.\n",
    "* Experiment with different loss functions: `mean_squared_error | logcosh | binary_crossentropy | cosine_proximity`\n",
    "* Experiment with different final layer nonlinearities: `softmax | elu | selu | relu | tanh | sigmoid | hard_sigmoid | linear`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Loss Functions: Jaccard\n",
    "Generally considered a powerful loss is also Jaccard loss; it provides larger errors and therefore more stable gradients close to the solution. $l_j = \\frac{\\sum |A*B|}{\\sum |A| +\\sum |B| -\\sum |A*B|}$\n",
    "\n",
    "In practice, we also have to prevent division by zero. The following code uses a smoothing term to avoid exploding or disapearing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def jaccard_distance_loss(y_true, y_pred, smooth=100):\n",
    "    \"\"\"\n",
    "    Jaccard = (|X & Y|)/ (|X|+ |Y| - |X & Y|)\n",
    "            = sum(|A*B|)/(sum(|A|)+sum(|B|)-sum(|A*B|))\n",
    "    The jaccard distance loss is useful for unbalanced datasets. This has been\n",
    "    shifted so it converges on 0 and is smoothed to avoid exploding or disapearing\n",
    "    gradient.\n",
    "    Ref: https://en.wikipedia.org/wiki/Jaccard_index\n",
    "    @url: https://gist.github.com/wassname/f1452b748efcbeb4cb9b1d059dce6f96\n",
    "    @author: wassname\n",
    "    \"\"\"\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return (1 - jac) * smooth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Train model with Jaccard loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = getModel()\n",
    "model.compile(loss=jaccard_distance_loss, optimizer='adam')\n",
    "print(\"Model parameters: {0:,}\".format(model.count_params()))\n",
    "\n",
    "padded_x_train, padding = pad_image_for_model(model, x_train[...,np.newaxis])\n",
    "history = model.fit(padded_x_train, \n",
    "                    y_train_binary[...,np.newaxis], \n",
    "                    batch_size=10, epochs=75, callbacks=[vh_callback]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Loss Functions: Dice\n",
    "* For segmentation, the Dice loss is also very common. $l_d = 2*\\sum \\frac{|A*B|} {\\sum A^2 + \\sum B^2}$\n",
    "\n",
    "NB: Jaccard and Dice are very similar overlap measures and can easily be computed from each other (bijection):\n",
    "<img src=\"images/jaccard_vs_dice.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    \"\"\"\n",
    "    Dice = (2*|X & Y|)/ (|X|+ |Y|)\n",
    "         =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))\n",
    "    ref: https://arxiv.org/pdf/1606.04797v1.pdf\n",
    "    @url: https://gist.github.com/wassname/7793e2058c5c9dacb5212c0ac0b18a8a\n",
    "    @author: wassname\n",
    "    \"\"\"\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    return (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1-dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Train model with Dice loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = getBNModel()\n",
    "model.compile(loss=dice_coef_loss, optimizer='adadelta')\n",
    "print(\"Model parameters: {0:,}\".format(model.count_params()))\n",
    "\n",
    "padded_x_train, padding = pad_image_for_model(model, x_train[...,np.newaxis])\n",
    "history = model.fit(padded_x_train,\n",
    "                    y_train_binary[...,np.newaxis],\n",
    "                    batch_size=10, epochs=75, callbacks=[vh_callback]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Segmentation using a U-Net\n",
    "* U-Nets are characterized by a downsampling path and an upsamling path, which allow for a pixel-wise output.\n",
    "* Skip connections are used between them in order to make it easier for the network to retain fine details.\n",
    "* Below is a diagram of the U-Net we will create now. You'll learn how to create it, too.\n",
    "<img src=\"images/U-net_4_levels.png\" alt=\"U-Net diagram\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# We will use this to generate the regularisation block for the sequential model.\n",
    "def addConvBNSequential(model, filters=32, kernel_size=(3,3), batch_norm=True, activation='prelu', padding='same', kernel_regularizer=None, name=None):\n",
    "    if batch_norm:\n",
    "        model = BatchNormalization()(model)\n",
    "    if activation == 'prelu':\n",
    "        model = Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation='linear', kernel_regularizer=kernel_regularizer, name=name)(model)\n",
    "        model = PReLU()(model)\n",
    "    elif activation == 'lrelu':\n",
    "        model = Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation='linear', kernel_regularizer=kernel_regularizer, name=name)(model)\n",
    "        model = LeakyReLU()(model)\n",
    "    else:\n",
    "        model = Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation=activation, kernel_regularizer=kernel_regularizer, name=name)(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Creates a small U-Net.\n",
    "from keras.layers import Input, concatenate\n",
    "def get_batchnorm_unet(_filters=32, _filters_add=0, _kernel_size=(3,3), _padding='same', _activation='prelu', _kernel_regularizer=None, _final_layer_nonlinearity='sigmoid', _batch_norm=True):\n",
    "\n",
    "    h, w = x_train.shape[1:]\n",
    "    if _padding == 'valid':\n",
    "        input_layer = Input(shape = (h+40, w+40, 1))\n",
    "    elif _padding == 'same':\n",
    "        input_layer = Input(shape = (h, w, 1))\n",
    "\n",
    "    x0 = addConvBNSequential(input_layer, filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm, name = 'firstConvolutionalLayer')\n",
    "    x0 = addConvBNSequential(x0,          filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
    "    x1 = MaxPool2D()(x0)\n",
    "    \n",
    "    x1 = addConvBNSequential(x1,          filters=_filters+_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
    "    x1 = addConvBNSequential(x1,          filters=_filters+_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
    "    x2 = MaxPool2D()(x1)\n",
    "    \n",
    "    x2 = addConvBNSequential(x2,          filters=_filters+2*_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
    "    x2 = addConvBNSequential(x2,          filters=_filters+2*_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
    "    x3 = UpSampling2D()(x2)\n",
    "    \n",
    "    x3 = concatenate([x1,x3])\n",
    "    x3 = addConvBNSequential(x3,          filters=_filters+_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
    "    x3 = addConvBNSequential(x3,          filters=_filters+_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
    "    x4 = UpSampling2D()(x3)\n",
    "    \n",
    "    x4 = concatenate([x0,x4])\n",
    "    x4 = addConvBNSequential(x4,          filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
    "    x4 = addConvBNSequential(x4,          filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
    "\n",
    "    output_layer = Conv2D(1, kernel_size=(1,1), activation=_final_layer_nonlinearity)(x4)\n",
    "    \n",
    "    model = Model(input_layer, output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = get_batchnorm_unet(_activation='relu', _batch_norm=True)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "print(\"Model parameters: {0:,}\".format(model.count_params()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It gets increasingly interesting to plot the architecture.\n",
    "# (1) plotting to PNG image file\n",
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='U-Net.png', show_shapes=False, show_layer_names=True)\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(filename = 'U-Net.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# (2) plotting to SVG vector graphics format\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train[...,np.newaxis],\n",
    "                    y_train_binary[...,np.newaxis],\n",
    "                    batch_size=10, epochs=75, callbacks=[vh_callback]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = get_batchnorm_unet(_activation='relu', _batch_norm=True)\n",
    "model.compile(loss=dice_coef_loss, optimizer='adam')\n",
    "print(\"Model parameters: {0:,}\".format(model.count_params()))\n",
    "history = model.fit(x_train[...,np.newaxis],\n",
    "                    y_train[...,np.newaxis],\n",
    "                    batch_size=10, epochs=75, callbacks=[vh_callback]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-Label Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert the labels into a one-hot representation\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# Convert to uint8 data and find out how many labels there are\n",
    "t = y_train.astype(np.uint8)\n",
    "t_max = int(np.max(y_test))\n",
    "print(\"Range of values: [0, {}]\".format(t_max))\n",
    "y_train_one_hot = to_categorical(t, num_classes=t_max+1).reshape((y_train.shape)+(t_max+1,))\n",
    "print(\"Shape before: {}; Shape after: {}\".format(y_train.shape, y_train_one_hot.shape))\n",
    "\n",
    "# The liver neuron should also be active for lesions within the liver\n",
    "liver = np.max(y_train_one_hot[:,:,:,1:], axis=3)\n",
    "y_train_one_hot[:,:,:,1] = liver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = getBNModel(_num_classes=4)\n",
    "model.compile(loss=dice_coef_loss, optimizer='adadelta')\n",
    "print(\"Model parameters: {0:,}\".format(model.count_params()))\n",
    "padded_x_train, padding = pad_image_for_model(model, x_train[...,np.newaxis])\n",
    "history = model.fit(padded_x_train,\n",
    "                    y_train_one_hot,\n",
    "                    batch_size=10, epochs=5, callbacks=[vh_callback]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Assignment: Extend the plot function to handle multiple classes.\n",
    "Then, activate the visualization callback in the training again. Try to find a slice with more than one output class to see the success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "// This code generates the table of contents at the top of the notebook\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "livereveal": {
   "auto_select": "code",
   "auto_select_fragment": "code",
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "default",
   "transition": "linear"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
