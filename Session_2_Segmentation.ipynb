{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mtwenzel/image-video-understanding/blob/master/Session_2_Segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_L9VrICrWifs"
   },
   "source": [
    "# Image and Video Understanding, Session 2: Segmentation\n",
    "\n",
    "In this notebook we want to train a network which segments the liver in CT data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlCCJJQvWifu"
   },
   "source": [
    "## 4. Preparing for Image Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KHT7pJJXWifw"
   },
   "outputs": [],
   "source": [
    "#@title Set TensorFlow Version to 2.x, and do basic imports { display-mode: \"form\" }\n",
    "#%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import InputLayer, Conv2D, MaxPool2D, Flatten, Dense, UpSampling2D, LocallyConnected2D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXblF5-GWif2"
   },
   "outputs": [],
   "source": [
    "#@title Check if a GPU is indeed available { display-mode: \"form\" }\n",
    "#@markdown Sometimes, it is useful to be able to check that tensorflow (which is used internally by keras here) is able to see a real GPU. If tensorflow falls back to the CPU, your code will still work, but be *much* slower.\n",
    "\n",
    "# print GPU device visible to tensorflow \n",
    "print('GPU:', tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7dk9CaGWif6"
   },
   "source": [
    "The first thing one needs to prepare is how to access the data:\n",
    "* separation of training & test data\n",
    "* random access to parts of the data (e.g. single images out of a large database)\n",
    "* random permutations (shuffling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtRSc5Txl1zg"
   },
   "source": [
    "### Download the data\n",
    "\n",
    "We have prepared a set of abdominal CT slices, strongly downsampled, completely anonymized, together with segmentation labels which we will inspect shortly. The following cell just downloads the data to the machine this notebook runs on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skhfff4kWif-"
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "test -e tmp_slices.npz || curl -L \"https://drive.google.com/uc?export=download&id=1R2-H0dhhrj6XNK7Q-MazIWGeFDOf6Zya\" --output tmp_slices.npz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKGCQnCymEie"
   },
   "source": [
    "### Loading the data\n",
    "\n",
    "Split into separate training and test sets.\n",
    "* Training converges with about 200 slices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "sc_llcAGWigB"
   },
   "outputs": [],
   "source": [
    "TRAINING_SLICE_COUNT = 600 #@param {min:100, max:3300, step:100}\n",
    "\n",
    "loaded = np.load('tmp_slices.npz')\n",
    "\n",
    "x_train = loaded['x_train'][:TRAINING_SLICE_COUNT]\n",
    "y_train = loaded['y_train'][:TRAINING_SLICE_COUNT]\n",
    "\n",
    "x_test = loaded['x_train'][TRAINING_SLICE_COUNT:]\n",
    "y_test = loaded['y_train'][TRAINING_SLICE_COUNT:]\n",
    "\n",
    "assert len(x_train) == len(y_train)\n",
    "print(\"Found %d training and %d testing slices\" % (len(x_train),len(x_test)))\n",
    "print(f\"The slices have shape {x_train.shape[1:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQTdaxb6WigE"
   },
   "source": [
    "### Inspecting the Demo Data\n",
    "The PNGs used for this hands-on contain abdominal CT scans showing the liver, resampled to 4mm voxel size and after applying a liver HU window (centered at 20 HU, width 450 HU).  The corresponding segmentatio masks contain values 0 (background), 1 (liver), 2..3 (different classes of lesions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xLveGV02WigF"
   },
   "outputs": [],
   "source": [
    "print(f'Labels found in the segmentation masks: {np.unique(y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPFk3lZWmSlN"
   },
   "source": [
    "### Select example slice and plot\n",
    "Have a look at different slices and their corresponding masks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "oQZ6_kA4WigJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example_test_slice = 1700 #@param {type:\"integer\"}\n",
    "\n",
    "f, ax = plt.subplots(1, 2, figsize = (9, 5))\n",
    "imgplot = ax[0].imshow(x_test[example_test_slice])\n",
    "ax[0].set_title('orig')\n",
    "imgplot = ax[1].imshow(y_test[example_test_slice])\n",
    "ax[1].set_title('mask')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBUm3XRrWigM"
   },
   "source": [
    "For now, we will start with a binary segmentation problem (0: background, 1: liver), so we will remove the lesion labels / turn them into liver (value 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LKDsXfCIWigN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove the lesion labels (values 2..3)\n",
    "y_train_binary = y_train.clip(0, 1)\n",
    "y_test_binary = y_test.clip(0, 1)\n",
    "\n",
    "f, ax = plt.subplots(1, 2, figsize = (9, 5))\n",
    "imgplot = ax[0].imshow(x_test[example_test_slice])\n",
    "ax[0].set_title('orig')\n",
    "imgplot = ax[1].imshow(y_test_binary[example_test_slice])\n",
    "ax[1].set_title('mask')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kl2F6vSBWigP"
   },
   "source": [
    "# 5. Segmentation: Auto Encoder (AE)-Style\n",
    "* We define short functions that return a model.\n",
    "* The first is a simple architecture that collapses and expands an image into the desired mask, similar to an Auto Encoder (AE).\n",
    "\n",
    "* We will exploit different configurations\n",
    "    * Regularisation\n",
    "    * Convolution mode: valid vs same\n",
    "    * Loss functions\n",
    "    \n",
    "* You can always play around with the parameters and change for example the batch size or number of epochs. When comparing two model which each other it is best to train both with similar configurations. Also when you want to train a model again do not forget to execute `model.compile()` again to get a fresh model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_DqSGa0WigQ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "tensorflow.keras.backend.image_data_format()\n",
    "tensorflow.keras.backend.set_image_data_format('channels_last')\n",
    "def getModel(_filters=32, filters_add=0, _kernel_size=(3,3), _padding='same', _activation='relu', _kernel_regularizer=None, _final_layer_nonlinearity='sigmoid'):\n",
    "    model = Sequential()\n",
    "    # We are indifferent about the xy size, but accept only one channel (gray value images). This has the consequence that debugging sizes gets harder.\n",
    "    model.add(InputLayer(input_shape=(None,None,1))) \n",
    "    \n",
    "    model.add(Conv2D(filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, name='firstConvolutionalLayer'))\n",
    "    model.add(Conv2D(filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer))\n",
    "    model.add(MaxPool2D())\n",
    "\n",
    "    model.add(Conv2D(filters=_filters+filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer))\n",
    "    model.add(Conv2D(filters=_filters+filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer))\n",
    "    model.add(MaxPool2D())\n",
    "\n",
    "    model.add(Conv2D(filters=_filters+2*filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer))\n",
    "    model.add(Conv2D(filters=_filters+2*filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer))\n",
    "    model.add(UpSampling2D())\n",
    "\n",
    "    model.add(Conv2D(filters=_filters+filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer))\n",
    "    model.add(Conv2D(filters=_filters+filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer))\n",
    "    model.add(UpSampling2D())\n",
    "\n",
    "    model.add(Conv2D(filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer))\n",
    "    model.add(Conv2D(filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer))\n",
    "\n",
    "    model.add(Conv2D(1, kernel_size=(1,1), activation=_final_layer_nonlinearity))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHgGtRBfWigf"
   },
   "source": [
    "### Executing code during training\n",
    "\n",
    "This is a small interludium section that defines some functions which will come in handy later on. \n",
    "\n",
    "In Keras (and also other frameworks), code can be executed with every iteration/batch/... Keras makes this particularly easy by offering \"callbacks\" where you can override one or more functions to execute your code. We will use this to generate images of the learning success after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R-fg-LzEWigg",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class VisualHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        # also show initial prediction\n",
    "        plot_prediction(self.model, example_test_slice)\n",
    "    \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        # show prediction after every training epoch\n",
    "        plot_prediction(self.model, example_test_slice)\n",
    "        \n",
    "vh_callback = VisualHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a specific configuration of the convolutions we need to pad our image before feeding it to the model. That means that we enlarge our image by adding pixels at the borders. We will see why that is necessary in the section about convolution models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_image_for_model(model, input_image):\n",
    "    '''Determine the necessary amount of padding\n",
    "    (difference between input and output size of the model)\n",
    "    and apply it to an ndarry with one or more images.'''\n",
    "    \n",
    "    padding = 0\n",
    "    if model.get_layer('firstConvolutionalLayer').padding == 'valid':\n",
    "        padding = 20 # WARNING: Hard-coded for above architecture!\n",
    "\n",
    "        # determine in which dimension to apply this padding\n",
    "        ndim_padding = []\n",
    "        if np.ndim(input_image) > 2:\n",
    "            # do not pad along batch dimension (if present)\n",
    "            ndim_padding.append((0, 0))\n",
    "        ndim_padding.append((padding, padding)) # pad above/below image (y dimension)\n",
    "        ndim_padding.append((padding, padding)) # pad left/right of image (x dimension)\n",
    "        if np.ndim(input_image) > 3:\n",
    "            # do not pad along channel dimension (if present)\n",
    "            ndim_padding.append((0, 0))\n",
    "        \n",
    "        input_image = np.lib.pad(input_image, ndim_padding,\n",
    "                                 #'constant', constant_values = 0)\n",
    "                                 'reflect')\n",
    "\n",
    "    return input_image, padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIRFjprvWigi"
   },
   "source": [
    "Once a model is trained, you'll want to evaluate it. Predicting using a given model and then plotting one slice is implemented in this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-1pu0nDWigj"
   },
   "outputs": [],
   "source": [
    "def do_prediction(model, input_image, verbose = False):\n",
    "    # first do padding of full slice\n",
    "    input_image, padding = pad_image_for_model(model, input_image)\n",
    "    \n",
    "    # add batch and channel dimensions (network expects 4D arrays)\n",
    "    input_array = input_image[np.newaxis,:,:,np.newaxis]\n",
    "    if verbose:\n",
    "        print(\"input shape:\", input_array.shape)\n",
    "    \n",
    "    # apply the model to the slice\n",
    "    y_predicted = model.predict(input_array)\n",
    "    if verbose:\n",
    "        print(\"output shape:\", y_predicted.shape)\n",
    "\n",
    "    return input_image, input_array, y_predicted, padding\n",
    "\n",
    "def plot_prediction(model, pred_slice_index):\n",
    "    # get single slice\n",
    "    input_image    = x_test[pred_slice_index]\n",
    "    # could use y_train_binary here for the first half of the notebook, but in the end we want to see the lesion\n",
    "    reference_mask = y_test[pred_slice_index]\n",
    "\n",
    "    input_image, input_array, y_predicted, padding = do_prediction(model, input_image)\n",
    "    \n",
    "    padded_extent = np.array([0, input_array.shape[2], input_array.shape[1], 0]) - 0.5 - padding\n",
    "\n",
    "    # display prediction for inspection\n",
    "    f, ax = plt.subplots(1, 5 if padding else 4, figsize = (11 if padding else 8, 3), sharey = True)\n",
    "    ax[0].imshow(x_test[pred_slice_index])\n",
    "    ax[0].set_title('orig')\n",
    "    if padding:\n",
    "        ax[1].imshow(input_array[0,:,:,0], extent = padded_extent)\n",
    "        ax[1].set_title('padded input')\n",
    "    ax[-2].imshow(y_predicted[0,:,:,0])\n",
    "    ax[-2].set_title('predicted mask')\n",
    "    ax[-3].imshow(reference_mask.clip(0,1))\n",
    "    ax[-3].set_title('reference mask')\n",
    "    ax[-1].imshow(reference_mask.clip(0,1) - y_predicted[0,:,:,0])\n",
    "    ax[-1].set_title('(ref - predicted)')\n",
    "    ax[0].set_ylim(*padded_extent[2:])\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = getModel()\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train \n",
    "history = model.fit(x_train[...,np.newaxis],\n",
    "                            y_train_binary[...,np.newaxis],\n",
    "                            batch_size=100, epochs=10, callbacks=[vh_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zuWbovHhWig0"
   },
   "source": [
    "## Prediction\n",
    "Let's look at the prediction from some more example slices, but let's only use the `x_test` slices that we did not use for training. (In a real scenario, we would do the separation of training & test data on the level of patients, *before* extracting slices, and we'd also have a validation set.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ThCB9hYCWig1"
   },
   "outputs": [],
   "source": [
    "# Randomly draw indices for slices \n",
    "slice_indices = np.random.choice(x_test.shape[0], 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFJeKo-TWig3"
   },
   "source": [
    "Plot some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-t1XTT4Wig4"
   },
   "outputs": [],
   "source": [
    "for a in slice_indices:\n",
    "    plot_prediction(model, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at the loss function\n",
    "fig,ax = plt.subplots(figsize=(18, 4), dpi= 80, facecolor='w', edgecolor='k')\n",
    "ax.plot(history.history['loss'])\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are you satisfied with your segmentation results? The next section about regularisation will offer some options for improvement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbDw7KajWihG"
   },
   "source": [
    "## 5.1 Regularisation\n",
    "* Regularisation should improve convergence. Let's try and add some Batch Normalisation first. Batch Normalisation intends to normalise the input to a (convolutional) layer, so that the values in the resulting feature maps don't get exessively large. \n",
    "    * Keras offers BatchNorm layers.\n",
    "    * Include one before each convolutional layer.\n",
    "* Another regularisation measure is to choose better activation functions. Probabilistic Rectified Linear Units (PReLU) crop negative values to a small epsilon, but route through any value greater than zero.\n",
    "    * In Keras, activation functions are selected through the `activation=[softmax|elu|selu|relu|tanh|sigmoid|hard_sigmoid|linear]` parameter to a layer, each in quotes.\n",
    "    * PReLU is one of the advanced activation functions that need to be added as a layer. It has many \"trainable\" parameters.\n",
    "* Lastly, L1 and L2 norm can be used as additional constraints on weights, biases, and activations.\n",
    "    * In Keras, this normalisation is again a parameter to the layer initialisation, using `kernel_regularizer=[l1(0.01)|l2(0.01)|l1_l2(0.01)]`.\n",
    "    * You have to `from keras.regularizers import l1, l2, l1_l2` to enable this functionality.\n",
    "    * You can also explore `bias_regularizer` and `activity_regularizer`.\n",
    "* Make your life easier by extracting a block: Conv -- Relu -- Batch Normalization. Then play with the options -- but carefully: When you just switch everything \"on\", chances are you will get bad results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gketrPXMWihG",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We will use the following block to generate the regularisation block\n",
    "\n",
    "from tensorflow.keras.layers import BatchNormalization, LeakyReLU, PReLU\n",
    "\n",
    "def addConvBN(model, filters=32, kernel_size=(3,3), batch_norm=True, activation='prelu', padding='same', kernel_regularizer=None, name = None):\n",
    "    if batch_norm:\n",
    "        model.add(BatchNormalization())\n",
    "    if activation == 'prelu':\n",
    "        model.add(Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation='linear', kernel_regularizer=kernel_regularizer, name = name))\n",
    "        model.add(PReLU())\n",
    "    elif activation == 'lrelu':\n",
    "        model.add(Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation='linear', kernel_regularizer=kernel_regularizer, name = name))\n",
    "        model.add(LeakyReLU())\n",
    "    else:\n",
    "        model.add(Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation=activation, kernel_regularizer=kernel_regularizer, name = name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZ3J_BJ6WihJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here we define the model including batch normalization layers\n",
    "def getBNModel(_filters=32, _filters_add=0, _kernel_size=(3,3), _padding='same', _activation='prelu', _kernel_regularizer=None, _final_layer_nonlinearity='sigmoid', _num_classes=1):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # this is really ugly, but TensorFlow's batch normalization\n",
    "    # currently has a limitation that it cannot work on unknown input sizes,\n",
    "    # so we need to get the height & width of our training data:\n",
    "    h, w = x_train.shape[1:]\n",
    "    if _padding == 'valid':\n",
    "        model.add(InputLayer(input_shape = (h+40, w+40, 1)))\n",
    "    elif _padding == 'same':\n",
    "        model.add(InputLayer(input_shape = (h, w, 1)))\n",
    "\n",
    "    addConvBN(model, filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, name='firstConvolutionalLayer')\n",
    "    addConvBN(model, filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)\n",
    "    model.add(MaxPool2D())\n",
    "\n",
    "    addConvBN(model, filters=_filters+_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)\n",
    "    addConvBN(model, filters=_filters+_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)\n",
    "    model.add(MaxPool2D())\n",
    "\n",
    "    addConvBN(model, filters=_filters+2*_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)\n",
    "    addConvBN(model, filters=_filters+2*_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)\n",
    "    model.add(UpSampling2D())\n",
    "\n",
    "    addConvBN(model, filters=_filters+_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)\n",
    "    addConvBN(model, filters=_filters+_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)\n",
    "    model.add(UpSampling2D())\n",
    "\n",
    "    addConvBN(model, filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)\n",
    "    addConvBN(model, filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)\n",
    "\n",
    "    model.add(Conv2D(_num_classes, kernel_size=(1,1), activation=_final_layer_nonlinearity))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eagqyx-lWihM"
   },
   "source": [
    "### Experiment 1: Batch Norm with PReLU\n",
    "The following model uses Batch Normalization and the PReLU layer. Note the number of parameters when executing model.summary() and compare them to the number for the network without Batch Norm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RXfOhBvTWihM",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This network uses the PReLU layer.\n",
    "modelBNPRelu = getBNModel(_padding='valid')\n",
    "modelBNPRelu.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "modelBNPRelu.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I3vQYcXxWihQ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "padded_x_train, padding = pad_image_for_model(modelBNPRelu, x_train[...,np.newaxis]) # why we do this will become clear later\n",
    "historyBNPRelu = modelBNPRelu.fit(padded_x_train,\n",
    "                                  y_train_binary[...,np.newaxis],\n",
    "                                  batch_size=10, epochs=10, callbacks=[vh_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the loss function for the model trained with and without Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbB_dADHWihT"
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(18, 4), dpi= 80, facecolor='w', edgecolor='k')\n",
    "ax.plot(history.history['loss'], label = 'without BN')\n",
    "ax.plot(historyBNPRelu.history['loss'], label = 'with batch normalization')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.grid()\n",
    "ax.legend(loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Batch Normalisation and PReLU, the number of parameters gets much larger, and training takes much longer. \n",
    "* Does the result warrant the wait?\n",
    "* You can have a look at predictions on the test set using `plot_prediction(modelName, sliceIndex)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the index to have a look at a different test image\n",
    "index = 200\n",
    "print(f'Prediction from model without BN')\n",
    "plot_prediction(model, index)\n",
    "print('Prediction for model with BN')\n",
    "plot_prediction(modelBNPRelu, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QC1KasVyWihY"
   },
   "source": [
    "### Experiment 2:  Batch Norm with ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the same model but using ReLu instead of PReLu. Compare the number of parameters and the performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HvXdV8Y2WihZ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "modelBNRelu = getBNModel(_padding='valid', _activation='relu')\n",
    "modelBNRelu.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "modelBNRelu.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XwWtueSjWihb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "padded_x_train, padding = pad_image_for_model(modelBNRelu, x_train[...,np.newaxis])\n",
    "historyBNRelu = modelBNRelu.fit(padded_x_train, y_train_binary[...,np.newaxis],\n",
    "                    batch_size=10, epochs=10, callbacks=[vh_callback]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(18, 4), dpi= 80, facecolor='w', edgecolor='k')\n",
    "ax.plot(history.history['loss'], label = 'without BN')\n",
    "ax.plot(historyBNPRelu.history['loss'], label = 'with batch normalization and PRelu')\n",
    "ax.plot(historyBNRelu.history['loss'], label = 'with batch normalization and Relu')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.grid()\n",
    "ax.legend(loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7v-X5SnCWihe"
   },
   "source": [
    "### Optional: Regularisation using L1/L2 norm on weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2MH552YuWihf",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.regularizers import l1, l2, l1_l2\n",
    "\n",
    "modelL2 = getModel(_padding='valid', _kernel_regularizer = l2(0.001))\n",
    "modelL2.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "print(\"Model parameters: {0:,}\".format(model.count_params()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_J6fFdirWihi"
   },
   "outputs": [],
   "source": [
    "padded_x_train, padding = pad_image_for_model(modelL2, x_train[...,np.newaxis])\n",
    "historyL2 = modelL2.fit(padded_x_train,\n",
    "                    y_train_binary[...,np.newaxis],\n",
    "                    batch_size=10, epochs=10, callbacks=[vh_callback]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0wvbiqFWigT"
   },
   "source": [
    "## 5.2 Convolution Mode 'valid' vs. 'same' / Automatic Padding\n",
    "\n",
    "To better understand the concept of convolutions we will have a look at two different modes - `'valid'` and `'same'` convolutions.  Convolutions are basically filters which slide accross the input image and combine the pixel values the filter sees. A 3x3 filter for example aggregates 9 values. But what happens at the boundary of the image? In `'valid'` mode the filter is not allowed to exceed the boundary of the image. Therefore the output of the convolution will be smaller than the input. In `'same'` mode the convolution operation will not alter the size of the image and therefor during convolution the image is padded (extended with a constant value). A description and visualization of `'valid'` and `'same'` convolutions can be found [here](https://towardsdatascience.com/the-most-intuitive-and-easiest-guide-for-convolutional-neural-network-3607be47480) under the section \"Convolution with padding and stride\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore the two modes using a dummy network which only performs one convolution on the input image - either in `'valid'` or in `'same'` mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_YAuRtVuWigU"
   },
   "outputs": [],
   "source": [
    "# We define a dummy model which only performs one convolution\n",
    "def getDummyConvModel(_filters=32, _filters_add=0, _kernel_size=(3,3), _padding='same', _activation='relu', _kernel_regularizer=None):\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=(None, None, 1)))\n",
    "    \n",
    "    model.add(Conv2D(filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get one dummy model with `padding = 'same'`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummyModelSame = getDummyConvModel(_padding='same')\n",
    "dummyModelSame.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and one dummy model with `padding = 'valid'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummyModelValid = getDummyConvModel(_padding='valid')\n",
    "dummyModelValid.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To expect the impact of the convolution mode we do not have to train our models. We just apply them to one image from the training set and inspect the output sizes. What can you observe? What impact could that have on the training? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputSame = dummyModelSame.predict(x_train[0:1,..., np.newaxis])\n",
    "outputValid = dummyModelValid.predict(x_train[0:1, ..., np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Output size for same convolution {outputSame.shape}')\n",
    "print(f'Output size for valid convolution {outputValid.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRF84i1HWigY"
   },
   "source": [
    "For our segmentation task we want an output image which classifies every pixel in the input image to belong to either foreground (liver) or background. When using `'valid'` convolutions the size of the output will be smaller compared to the input, so when just feeding the input image as it is we will get a segmentation only for a part of the image. To prevent that we can enlarge our input image by padding so that still an output corresponding to the full slice (or the patch size, respectively) is generated.\n",
    "The number of pixels to pad depends on the network architecture. There is an excellent technical report detailing the arithmetics to calculate the receptive field and required padding from a network definition at https://arxiv.org/pdf/1603.07285.pdf. Another description is at https://medium.com/@Synced/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-42f33d4378e0.\n",
    "\n",
    "The function to pad the image is somewhat hard-coded for the network architecture above: it pads the input with a fixed amount of voxels in case the model gives an output that is differently sized from the inmput.\n",
    "\n",
    "Now we try the different convolution modes with our full model: \n",
    "\n",
    "### Experiment 1: 'valid' Convolutions\n",
    "\n",
    "Get a model with padding of 'valid', i.e. the size will shrink and a proper amount of voxels need to be added prior to processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F365L4IOWigY",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelValid = getBNModel(_padding='valid', _activation='relu')\n",
    "\n",
    "modelValid.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "modelValid.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EUGAsM7TWigb"
   },
   "source": [
    "### Experiment 2: 'same' Convolutions\n",
    "\n",
    "Get a model with `same` padding, i.e. the input is padded with zeros before each convolution such that an output size equal to the input is achieved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MGzqDeXRWigc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelSame = getBNModel(_activation='relu')\n",
    "\n",
    "modelSame.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "modelSame.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUisOPkQWigp"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model with 'valid' convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBMzxQOiWigx"
   },
   "outputs": [],
   "source": [
    "# We first pad the images before feeding them to the network\n",
    "padded_x_train, padding = pad_image_for_model(modelValid, x_train[...,np.newaxis])\n",
    "historyValid = modelValid.fit(padded_x_train,\n",
    "                              y_train_binary[...,np.newaxis],\n",
    "                              batch_size=10, epochs=10, callbacks=[vh_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model with 'same' convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4uzGk9gOWigs"
   },
   "outputs": [],
   "source": [
    "# This is for _padding = 'same'\n",
    "historySame = modelSame.fit(x_train[...,np.newaxis],\n",
    "                            y_train_binary[...,np.newaxis],\n",
    "                            batch_size=10, epochs=10, callbacks=[vh_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zuWbovHhWig0"
   },
   "source": [
    "## Prediction\n",
    "Let's look at the prediction from some more example slices from `x_test` slices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ThCB9hYCWig1"
   },
   "outputs": [],
   "source": [
    "slice_indices = np.random.choice(x_test.shape[0], 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFJeKo-TWig3"
   },
   "source": [
    "### Evaluate model with 'valid' convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-t1XTT4Wig4"
   },
   "outputs": [],
   "source": [
    "for a in slice_indices:\n",
    "    plot_prediction(modelValid, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIgnQS5PWig6"
   },
   "source": [
    "### Evaluate model with 'same' convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXTA7tIGWig8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for a in slice_indices:\n",
    "    plot_prediction(modelSame, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GaZpDTr7Wig-"
   },
   "source": [
    "### Advanced: Tile-based Prediction\n",
    "\n",
    "For our small data, it is possible to train a classifier that takes full slices. However, when dealing with input of arbitrary size, this will no longer work. Images have to be partitioned, and the individual results need to be stitched into the final output.\n",
    "\n",
    "Let's predict the image divided into upper and lower half, and then in full. Note that the network could also predict smaller or larger tile sizes.  Such an approach is necessary for large images, or with volumetric data and 3D convolutions, when it is not possible to have the whole image in (GPU) memory at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k8PZwtyGWig-"
   },
   "outputs": [],
   "source": [
    "# hand-picked slice useful to visualize benefits of tiled approach\n",
    "example_test_slice = 909 - TRAINING_SLICE_COUNT\n",
    "assert example_test_slice > 0, 'when increasing TRAINING_SLICE_COUNT, please pick a new example slice as well'\n",
    "\n",
    "\n",
    "def plot_prediction_tiled(model, pred_slice_index, tile = None):\n",
    "    # get single slice\n",
    "    input_image    = x_train[pred_slice_index]\n",
    "    reference_mask = y_train_binary[pred_slice_index]\n",
    "\n",
    "    # first do padding of full slice\n",
    "    input_image, padding = pad_image_for_model(model, input_image)\n",
    "    \n",
    "    # add batch and channel dimensions (network expects 4D arrays)\n",
    "    input_array = input_image[np.newaxis,:,:,np.newaxis]\n",
    "    print(\"padded input shape:\", input_array.shape)\n",
    "\n",
    "    # then cut the specified tile box (plus padding)\n",
    "    if tile is not None: # (default is full slice)\n",
    "        tile = np.asarray(tile)\n",
    "        assert tile.shape == (2, 2)\n",
    "        padded_tile = tile.copy()\n",
    "        padded_tile[:,1] += 2*padding\n",
    "        input_array = input_array[:,\n",
    "                                  padded_tile[0][0]:padded_tile[0][1],\n",
    "                                  padded_tile[1][0]:padded_tile[1][1],\n",
    "                                  :]\n",
    "        reference_mask = reference_mask[tile[0][0]:tile[0][1],\n",
    "                                        tile[1][0]:tile[1][1]]\n",
    "        print(\"tiled padded shape:\", input_array.shape)\n",
    "\n",
    "    y_predicted = model.predict(input_array)\n",
    "    print(\"output shape:\", y_predicted.shape)\n",
    "\n",
    "    padded_extent = np.array([0,input_array.shape[2],input_array.shape[1],0]) - 0.5 - padding\n",
    "    orig_extent = np.array([0,reference_mask.shape[1],reference_mask.shape[0],0]) - 0.5\n",
    "    if tile is not None:\n",
    "        padded_extent[:2] += tile[1,0]\n",
    "        padded_extent[2:] += tile[0,0]\n",
    "        orig_extent[:2] += tile[1,0]\n",
    "        orig_extent[2:] += tile[0,0]\n",
    "\n",
    "    # display prediction for inspection\n",
    "    f, ax = plt.subplots(1, 4 if padding else 3, figsize = (14 if padding else 12, 3), sharey = True)\n",
    "    ax[0].imshow(x_train[pred_slice_index])\n",
    "    ax[0].set_title('orig')\n",
    "    if padding:\n",
    "        ax[1].imshow(input_array[0,:,:,0], extent = padded_extent)\n",
    "        ax[1].set_title('padded input')\n",
    "    ax[-2].imshow(y_predicted[0,:,:,0], extent = orig_extent)\n",
    "    ax[-2].set_title('predicted mask')\n",
    "    ax[-1].imshow(reference_mask, extent = orig_extent)\n",
    "    ax[-1].set_title('reference mask')\n",
    "    ax[0].set_ylim(*padded_extent[2:])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sI1TDmpAWihA",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tile = [[0,42],[0,76]]\n",
    "plot_prediction_tiled(model, example_test_slice, tile)\n",
    "tile = [[42,76],[0,76]]\n",
    "plot_prediction_tiled(model, example_test_slice, tile)\n",
    "plot_prediction(model, example_test_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h13gYlpHWihl"
   },
   "source": [
    "# 6. Loss functions\n",
    "* Loss functions take the predicted output, `y_pred`, and the expected output, `y_train`, and calculate their distance. The result is the minimization target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEIbsDocWihm"
   },
   "source": [
    "## Loss functions: Final layer non-linearity dependency\n",
    "* We have worked with binary crossentropy. See [this blog series](http://neuralnetworksanddeeplearning.com/chap3.html) for a comment:\n",
    "* Optional: If you want you can use the models above and: \n",
    "    * Experiment with different loss functions: `mean_squared_error | logcosh | binary_crossentropy | cosine_proximity` (the loss function is an argument in model.compile())\n",
    "    * Experiment with different final layer nonlinearities: `softmax | elu | selu | relu | tanh | sigmoid | hard_sigmoid | linear` (you can adapt that in the model definition: model = getBNModel(_final_layer_nonlinearity='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kU8QCRGhWihm"
   },
   "source": [
    "## Loss Functions: Jaccard\n",
    "Generally considered a powerful loss is also Jaccard loss; it provides larger errors and therefore more stable gradients close to the solution. $l_j = \\frac{\\sum |A*B|}{\\sum |A| +\\sum |B| -\\sum |A*B|}$\n",
    "\n",
    "In practice, we also have to prevent division by zero. The following code uses a smoothing term to avoid exploding or disapearing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7WMDkeIRWihn",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def jaccard_distance_loss(y_true, y_pred, smooth=100):\n",
    "    \"\"\"\n",
    "    Jaccard = (|X & Y|)/ (|X|+ |Y| - |X & Y|)\n",
    "            = sum(|A*B|)/(sum(|A|)+sum(|B|)-sum(|A*B|))\n",
    "    The jaccard distance loss is useful for unbalanced datasets. This has been\n",
    "    shifted so it converges on 0 and is smoothed to avoid exploding or disapearing\n",
    "    gradient.\n",
    "    Ref: https://en.wikipedia.org/wiki/Jaccard_index\n",
    "    @url: https://gist.github.com/wassname/f1452b748efcbeb4cb9b1d059dce6f96\n",
    "    @author: wassname\n",
    "    \"\"\"\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return (1 - jac) * smooth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_mbfddWWihq"
   },
   "source": [
    "### Train model with Jaccard loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mU7KBayUWiht",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "modelJaccard = getBNModel() # Using a non-regularized model (without BN) will likely not converge.\n",
    "modelJaccard.compile(loss=jaccard_distance_loss, optimizer='adam')\n",
    "print(\"Model parameters: {0:,}\".format(model.count_params()))\n",
    "\n",
    "padded_x_train, padding = pad_image_for_model(modelJaccard, x_train[...,np.newaxis])\n",
    "historyJaccard = modelJaccard.fit(padded_x_train, \n",
    "                    y_train_binary[...,np.newaxis], \n",
    "                    batch_size=36, epochs=30, callbacks=[vh_callback]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the trained model and its predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curve\n",
    "fig,ax = plt.subplots(figsize=(18, 4), dpi= 80, facecolor='w', edgecolor='k')\n",
    "ax.plot(historyJaccard.history['loss'])\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a prediction\n",
    "prediction_index = 10 #@param {type:'integer'}\n",
    "plot_prediction(modelJaccard, prediction_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is the model performance compared to the other models? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlMeMuy8Wihw"
   },
   "source": [
    "## Loss Functions: Dice\n",
    "* For segmentation, the Dice loss is also very common. $l_d = 2*\\sum \\frac{|A*B|} {\\sum A^2 + \\sum B^2}$\n",
    "\n",
    "NB: Jaccard and Dice are very similar overlap measures and can easily be computed from each other (bijection):\n",
    "<img src=\"https://github.com/mtwenzel/image-video-understanding/blob/master/images/jaccard_vs_dice.png?raw=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7a7ft2U9Wihw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    \"\"\"\n",
    "    Dice = (2*|X & Y|)/ (|X|+ |Y|)\n",
    "         =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))\n",
    "    ref: https://arxiv.org/pdf/1606.04797v1.pdf\n",
    "    @url: https://gist.github.com/wassname/7793e2058c5c9dacb5212c0ac0b18a8a\n",
    "    @author: wassname\n",
    "    \"\"\"\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    return (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1-dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77W3SgmCWihy"
   },
   "source": [
    "### Train model with Dice loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SGL-u4bWihz",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelDice = getBNModel()\n",
    "modelDice.compile(loss=dice_coef_loss, optimizer='adadelta')\n",
    "print(\"Model parameters: {0:,}\".format(model.count_params()))\n",
    "\n",
    "padded_x_train, padding = pad_image_for_model(modelDice, x_train[...,np.newaxis])\n",
    "historyDice = modelDice.fit(padded_x_train,\n",
    "                    y_train_binary[...,np.newaxis],\n",
    "                    batch_size=36, epochs=30, callbacks=[vh_callback]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curve and compare with Jaccard\n",
    "fig,ax = plt.subplots(figsize=(18, 4), dpi= 80, facecolor='w', edgecolor='k')\n",
    "ax.plot(historyJaccard.history['loss'], label='Jaccard')\n",
    "ax.plot(historyDice.history['loss'], label='Jaccard')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a prediction\n",
    "prediction_index = 10 #@param {type:'integer'}\n",
    "plot_prediction(modelDice, prediction_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJMQhEEkWih1"
   },
   "source": [
    "# Segmentation using a U-Net\n",
    "* U-Nets are characterized by a downsampling path and an upsamling path, which allow for a pixel-wise output.\n",
    "* Skip connections are used between them in order to make it easier for the network to retain fine details.\n",
    "* Below is a diagram of the U-Net we will create now. You'll learn how to create it, too.\n",
    "<img src=\"https://github.com/mtwenzel/image-video-understanding/blob/master/images/U-net_4_levels.png?raw=1\" alt=\"U-Net diagram\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2P1gPdxWih2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We will use this to generate the regularisation block for the sequential model.\n",
    "def addConvBNSequential(model, filters=32, kernel_size=(3,3), batch_norm=True, activation='prelu', padding='same', kernel_regularizer=None, name=None):\n",
    "    if batch_norm:\n",
    "        model = BatchNormalization()(model)\n",
    "    if activation == 'prelu':\n",
    "        model = Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation='linear', kernel_regularizer=kernel_regularizer, name=name)(model)\n",
    "        model = PReLU()(model)\n",
    "    elif activation == 'lrelu':\n",
    "        model = Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation='linear', kernel_regularizer=kernel_regularizer, name=name)(model)\n",
    "        model = LeakyReLU()(model)\n",
    "    else:\n",
    "        model = Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation=activation, kernel_regularizer=kernel_regularizer, name=name)(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0P1DYh_QWih5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creates a small U-Net.\n",
    "from tensorflow.keras.layers import Input, concatenate\n",
    "def get_batchnorm_unet(_filters=32, _filters_add=0, _kernel_size=(3,3), _padding='same', _activation='prelu', _kernel_regularizer=None, _final_layer_nonlinearity='sigmoid', _batch_norm=True):\n",
    "\n",
    "    h, w = x_train.shape[1:]\n",
    "    if _padding == 'valid':\n",
    "        input_layer = Input(shape = (h+40, w+40, 1))\n",
    "    elif _padding == 'same':\n",
    "        input_layer = Input(shape = (h, w, 1))\n",
    "\n",
    "    x0 = addConvBNSequential(input_layer, filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm, name = 'firstConvolutionalLayer')\n",
    "    x0 = addConvBNSequential(x0,          filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
    "    x1 = MaxPool2D()(x0)\n",
    "    \n",
    "    x1 = addConvBNSequential(x1,          filters=_filters+_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
    "    x1 = addConvBNSequential(x1,          filters=_filters+_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
    "    x2 = MaxPool2D()(x1)\n",
    "    \n",
    "    x2 = addConvBNSequential(x2,          filters=_filters+2*_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
    "    x2 = addConvBNSequential(x2,          filters=_filters+2*_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
    "    x3 = UpSampling2D()(x2)\n",
    "    \n",
    "    x3 = concatenate([x1,x3])\n",
    "    x3 = addConvBNSequential(x3,          filters=_filters+_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
    "    x3 = addConvBNSequential(x3,          filters=_filters+_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
    "    x4 = UpSampling2D()(x3)\n",
    "    \n",
    "    x4 = concatenate([x0,x4])\n",
    "    x4 = addConvBNSequential(x4,          filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
    "    x4 = addConvBNSequential(x4,          filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
    "\n",
    "    output_layer = Conv2D(1, kernel_size=(1,1), activation=_final_layer_nonlinearity)(x4)\n",
    "    \n",
    "    model = Model(input_layer, output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0hdO7vYWih8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelUNet = get_batchnorm_unet(_activation='relu', _batch_norm=True)\n",
    "modelUNet.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "print(\"Model parameters: {0:,}\".format(model.count_params()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EGlmf5D0Wih9"
   },
   "outputs": [],
   "source": [
    "# It gets increasingly interesting to plot the architecture.\n",
    "# (1) plotting to PNG image file\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(modelUNet, to_file='U-Net.png', show_shapes=False, show_layer_names=True)\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(filename = 'U-Net.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PK5fzljGWiiD",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "historyUNet = modelUNet.fit(x_train[...,np.newaxis],\n",
    "                    y_train_binary[...,np.newaxis],\n",
    "                    batch_size=10, epochs=30, callbacks=[vh_callback]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can train the U-net also with other loss functions like the dice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZauCRhzXWiiF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelUNetDice = get_batchnorm_unet(_activation='relu', _batch_norm=True)\n",
    "modelUNetDice.compile(loss=dice_coef_loss, optimizer='adam')\n",
    "print(\"Model parameters: {0:,}\".format(model.count_params()))\n",
    "historymodelUNetDice = modelUNetDice.fit(x_train[...,np.newaxis],\n",
    "                    y_train[...,np.newaxis],\n",
    "                    batch_size=10, epochs=30, callbacks=[vh_callback]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want you can plot the loss of the UNet or look at predictions on the test set just like we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Room for further code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Og9RawOOWiiH"
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "// This code generates the table of contents at the top of the notebook\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Session_2_Segmentation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
