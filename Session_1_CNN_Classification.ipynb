{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mtwenzel/image-video-understanding/blob/master/Session_1_CNN_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTDP7DB926WP"
   },
   "source": [
    "### This is a Jupyter notebook.  \n",
    "### The most important keyboard shortcuts (cf. the \"Help\" menu) are\n",
    "* **cursor keys** to select cells\n",
    "* **Enter** to go from command mode to edit mode (for changing cell contents)\n",
    "  * (**Esc** would go back to command mode.)\n",
    "* **Shift+Enter** to *execute and advance* a cell\n",
    "    \n",
    "### If you execute it in Google Colab, some extra functions are provided:\n",
    "\n",
    "* Cells can **hide the code**. This is the case for the \"Imports\" cell below. Double-clicking still gets you to the code directly. The code can be hidden again with a double-click.\n",
    "* Cells can provide convenient **parameter interfaces**, like drop-down lists, sliders, and input fields. You will see this in the \"Initialize random data\" cell below. Again, double-clicking brings up the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tY6dUFRv26WR"
   },
   "source": [
    "# Image and Video Understanding -- Session 1 (Classification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eki0pqq026WS"
   },
   "source": [
    "## 1. First Experiments with Random Data\n",
    "* Start by importing some required python modules that implement the layers we will use to build the network. \n",
    "* We also need a \"container\" to connect the layers: the \"Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "b7wllEz626WT"
   },
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "#@markdown To edit the imports, double-click on the cell\n",
    "\n",
    "#@markdown We set TensorFlow 2.x as default for this notebook. This includes Keras.\n",
    "\n",
    "#%tensorflow_version 2.x\n",
    "\n",
    "from tensorflow.keras.layers import InputLayer, Conv2D, MaxPool2D, Flatten, Dense, UpSampling2D, LocallyConnected2D, Dropout\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_amAT5l26WW"
   },
   "source": [
    "### Create random data\n",
    "\n",
    "In these examples, we'll use artificial data first, and then switch to real data.\n",
    "\n",
    "Run the code in the following cell, which will create a pair of input data `x_train` and corresponding output data `y_train` for training a classifier.  `x_train` contains the set number of training examples (or instances), with the set number of features. `y_train` contains labels, with the first half `1`, and the second `0`. Our goal is to train a model which takes the input data `x` and map them to one of the two classes `0` or `1` contained in `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ILE5IZO726WW"
   },
   "outputs": [],
   "source": [
    "#@title Initialize random data\n",
    "#@markdown Create random data sampled from uniform distribution.\n",
    "#@markdown Set the desired number of instances\n",
    "NUM_INSTANCES = 100 #@param {type:'slider', min:0, max:10000, step:100}\n",
    "#@markdown Set the desired number of features (random from uniform distribution)\n",
    "NUM_FEATURES = 1000 #@param {type:'slider', min:0, max:10000, step:100}\n",
    "x_train = np.random.random((NUM_INSTANCES, NUM_FEATURES)) \n",
    "y_train = np.zeros((NUM_INSTANCES,)) # Label vector (initialized with 0s)\n",
    "y_train[:int(NUM_INSTANCES/2)] = 1 # set first half of vector to 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "We define the deep learning model below in a _sequential_ fashion, which means that the single layers are added one after another. Here we use a simple fully-connected model using `Dense` layers. Each feature from the input vector `x_train` is connected to every neuron (called unit) in the following `Dense` layer. In the input layer we must know the shape of our data which will be fed to the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1b398tpb26WZ"
   },
   "outputs": [],
   "source": [
    "model = Sequential() # We choose a simple sequential model without branching\n",
    "model.add(InputLayer(input_shape = (NUM_FEATURES,)))\n",
    "#@markdown Play with the number of neurons\n",
    "NUM_NEURONS = 256 #@param {type:'integer'}\n",
    "model.add(Dense(units=NUM_NEURONS, name=\"Hidden\")) \n",
    "\n",
    "#@markdown Optionally increase the number of layers.\n",
    "#model.add(Dense(units=128))\n",
    "#model.add(Dense(units=64))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adadelta')\n",
    "model.summary()\n",
    "# @markdown If only interested in the number of parameters, use this:\n",
    "# @markdown `print(\"Model parameters: {0:,}\".format(model.count_params()))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8bU6j7r26Wb"
   },
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network by executing the following. \n",
    "\n",
    "Clicking left to the output once will change the display mode from a scrollable field to a full display and back. Double-clicking it collapses it, so it is not so dominant.\n",
    "In Google Colab, you can savely `x` the output with a click in the top left corner. This removes the printout, but not the cell results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1s0plRmN26Wc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=10, epochs=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate the \"history\" object you created\n",
    "The training stores its history and important parameters in the _history_ parameter we assigned it to. \n",
    "* Try out the following commands and inspect the variables.\n",
    "* Make use of tab completion, e.g. by typing `hidden_layer.` and press `<tab>` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kEePRAbJ26Wf"
   },
   "outputs": [],
   "source": [
    "loss_history = history.history['loss']\n",
    "print(f'Loss history: {loss_history}')\n",
    "weights = history.model.get_weights()\n",
    "hidden_layer = history.model.get_layer(\"Hidden\")\n",
    "for w in weights:\n",
    "    print(w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can also display the learning success as measured by the loss by plotting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(18, 4), dpi= 80, facecolor='w', edgecolor='k')\n",
    "ax.plot(history.history['loss'])\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nz8lrfL726We"
   },
   "source": [
    "### Interpreting the result\n",
    "* What can you observe regarding the loss?\n",
    "* Why is that possible?\n",
    "* Change the number of training instances to 1000. Assure that the classes are equally frequent again. What can you observe?\n",
    "* Be reminded that you have to re-create the model to reset the weights. To do this, execute the cell with the model definition (important is the `model.compile()` call)\n",
    "\n",
    "\n",
    "### A remark on optimization\n",
    "* Optimizers like SGD, ADAM, ADAGrad ADADelta etc. are variants of Stochastic Gradient Descent (SGD).\n",
    "* SGD estimates the gradient for parameters based on a batch of examples.\n",
    "    * The larger the batch, the better the estimated gradiend approximates the gradient for the whole dataset.\n",
    "* It takes about 300 epochs to converge when creating 1000 instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZR1farX26Wh"
   },
   "source": [
    "## 2. Image Classification: _MNIST handwritten digits_\n",
    "\n",
    "### Read the data\n",
    "\n",
    "* We want to work on images: MNIST is a public dataset which contains images of handwritten digits. \n",
    "* You can import them from Keras with one line, because it is one of the standard datasets used for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Dj5ic03p26Wi"
   },
   "outputs": [],
   "source": [
    "#@title Import MNIST data\n",
    "#@markdown If you execute this cell, you will overwrite the data `x_train` and `y_train` above. In addition, it gives you test data in `x_test` and `y_test`.\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reduce data by factor 10 / 20 for fast execution during course\n",
    "x_train = x_train[::10]\n",
    "y_train = y_train[::10]\n",
    "x_test = x_test[::20]\n",
    "y_test = y_test[::20]\n",
    "\n",
    "# verify resulting array shapes\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hftKUCa26Wk"
   },
   "source": [
    "### Inspecting the data\n",
    "\n",
    "Look at the shape of the `x_train` variable to understand how the data is organised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the shape of x_train\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can see that the data has 6000 training examples, each of shape 28x28.\n",
    "* These are images of size 28x28 pixels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the shape and values of `y_train` to understand the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " print(f'Shape of y: {y_train.shape}, Minimum of y: {y_train.min()}, Maximum of y: {y_train.max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The corresponding output is just a long vector of corresponding labels in the range [0...9], refering to the displayed digit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "syYfQPY626Wo"
   },
   "source": [
    "As we are dealing with *images* now, we want to display them.\n",
    "* `matplotlib` is a python package well suited plotting data and displaying images.\n",
    "* Change the index of `x_train[index]` in the cell below to have a look at different images from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EdZLsaFq26Wo"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckDwKNqc26Wq"
   },
   "outputs": [],
   "source": [
    "# Look at an image\n",
    "plt.imshow(x_train[700], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NmZwqTQ26Wv"
   },
   "source": [
    "Also, we could be interested in the distribution of labels in our data so we plot a histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2BoOyAKY26Ww"
   },
   "outputs": [],
   "source": [
    "plt.hist(y_train)\n",
    "plt.xlabel('Digits')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vD59O5wq26Wy"
   },
   "source": [
    "### Preparing the labels for a classification network\n",
    "We want to convert the numeric labels to so-called *\"one-hot vectors\"*.\n",
    "* One-hot means that the network does not directly output a number between 0 and 9 representing the digit.\n",
    "* Rather, we want a vector with 10 entries, in which only one entry is 1, all others 0, e.g. `[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]` to label a \"2\".\n",
    "* *Rationale:* The digits represent different categorical classes, and we want to penalize all confused digits the same; it is not \"better\" or \"closer\" if the network outputs 4.2 given an image depicting a \"6\" than if the output is 1.\n",
    "* In general, the one-hot encoding helps with classification problems and allows to let the neuron with maximal activation \"win\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "muMO1Ar626Wz"
   },
   "outputs": [],
   "source": [
    "num_labels = 10\n",
    "# Code to convert labels\n",
    "y_train_one_hot = (np.arange(num_labels) == y_train[:,np.newaxis]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7nxi5-3Q26W1"
   },
   "outputs": [],
   "source": [
    "# Keras offers a convenience function to achieve the same:\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=num_labels)\n",
    "# Same for the testing data\n",
    "y_test_one_hot  = to_categorical(y_test, num_classes=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the shape of the one-hot-converted vector `y_train_one_hot` to assure that each training example no has 10 entries. Compare the digit representation exemplarily to the one-hot-representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train_one_hot.shape)\n",
    "print(f'Digit {y_train[10]} is converted to {y_train_one_hot[10,:]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huoaBC6D26W3"
   },
   "source": [
    "### Image classification with a simple neural network\n",
    "We now want to train the above network on this data. It should take the images in `x_train` as input and predicts the correct digit as stored in `y_train_one_hot`. We have to adapt the model to use inputs of 28 x 28, and to produce vector outputs. We have prepared this below:\n",
    "* Modified the parameter `input_shape=(...)` to adapt to the new data\n",
    "* Modified the number of dense units in the output layer to reflect the number of classes; 10 in the digits example\n",
    "* Modified the loss function to deal with multiple classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zE0crVQE26W4"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(28,28)))\n",
    "model.add(Flatten()) # Layer reshaping the 28x28 arrays into vectors of length 28*28=784\n",
    "model.add(Dense(units=128)) # Try higher or lower numbers of hidden units!\n",
    "# Try adding more layers!\n",
    "model.add(Dense(units=128))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#model.add(Dense(units=128))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(units=10, activation='softmax', name='output')) # The number of units in the output layer refers to the number if classes\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adadelta') # The categorical crossentropy loss can deal with multiple classes\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kBqoD79p26W6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This experiments takes about 1 sec per epoch on an older MacBook Pro.\n",
    "history = model.fit(x_train, y_train_one_hot, batch_size=500, epochs=100) # In this example, you'll no longer want batches of size 10..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CutYDwpj26W9"
   },
   "source": [
    "### Evaluate the model on independent test data\n",
    "* The following cell executes the model on the test data using `model.predict()`\n",
    "* The result is a list of 10-vectors (recall the on-hot encoding), only this time there are also values between 0 and 1.\n",
    "* How can we compare these with the true labels in `y_test_one_hot`? There are many possible ways to evaluate classifiers; in general, you want to define some kind of error, usually based on differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UViKNdsj26W-"
   },
   "outputs": [],
   "source": [
    "pred = model.predict(x_test)\n",
    "print(f'Shape of the test input: {x_test.shape}, shape of the predicted output: {pred.shape}')\n",
    "print(f'Exemplary prediction: {pred[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5A03cM7T26XC"
   },
   "source": [
    "The `argmax()` function may come in handy, which converts from the one-hot representation back to integer indices of the maximally activated classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HghimxdO26XD"
   },
   "outputs": [],
   "source": [
    "pred_integer_indices = pred.argmax(axis = -1)\n",
    "print(f'Exemplary prediction: {pred_integer_indices[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the prediction of our network on the test dataset: How well does this prediction fit the real classes of the data? Let's compare the predictions with the true classes `y_true`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = y_test - pred_integer_indices\n",
    "correctly_classified_examples = np.where(diff == 0)[0].shape[0]\n",
    "num_examples = y_test.shape[0]\n",
    "wrongly_classified_examples = num_examples - correctly_classified_examples\n",
    "print(f'{correctly_classified_examples} ({correctly_classified_examples/num_examples * 100} %) of the examples are classified correctly while {wrongly_classified_examples} ({wrongly_classified_examples/num_examples * 100}%) are classified wrong.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are you satisfied with the performance of the classifier? If not, play around with the parameters and try to get a better result. You can for example do one of the following\n",
    "* Take a look at the loss function using the code in the following cell:\n",
    "    * Has the model finished it's training? You can increase the number of epochs in the model training above. \n",
    "    * Increase the batch size of the training.\n",
    "    \n",
    "* Increase the number of neurons (units) in the model. \n",
    "* Add more layers to you model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(18, 4), dpi= 80, facecolor='w', edgecolor='k')\n",
    "ax.plot(history.history['loss'])\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUGW5DKC26XF"
   },
   "source": [
    "## 3. Image classification with a simple convolutional neural network (CNN)\n",
    "We now try to solve the MNIST classification using a convolutional neural network instead of a fully-connected network. \n",
    "\n",
    "### Define the network\n",
    "* Instead of using `Dense` layers as before we now use `Conv2D` layers. These layers perform convolutions accross their input (you can find a visualization of this [here](https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d)). One can view this convolution operation also as a filtering with different kernels. The number of filters which are learned and the size of these filters are a parameter of the `Conv2D` layer. By filtering for example with 32 filters one add a new dimension to the data which is often also called the \"channel dimension\" (after filtering the images are of size 128, 128, 32).\n",
    "* To learn patterns on different spatial resolutions we downsample between the convolutions using the `MaxPool2D` layer which only keeps the maximum value in a 2x2 neighborhood.  \n",
    "* To remove the 2D nature again and end up with an output vector of 10 entries we use `Flatten()` which just converts the 2D image in a 1D vector by writing all values in one row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "tensorflow.keras.backend.image_data_format()\n",
    "tensorflow.keras.backend.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vpzYN06B26XJ"
   },
   "outputs": [],
   "source": [
    "convnet = Sequential()\n",
    "convnet.add(InputLayer(input_shape=(28,28,1)))\n",
    "convnet.add(Conv2D(filters=32, kernel_size=(3,3), padding='same'))\n",
    "convnet.add(Conv2D(filters=32, kernel_size=(3,3), padding='same'))\n",
    "convnet.add(MaxPool2D())\n",
    "convnet.add(Conv2D(filters=32, kernel_size=(3,3), padding='same'))\n",
    "convnet.add(Conv2D(filters=32, kernel_size=(3,3), padding='same'))\n",
    "convnet.add(MaxPool2D())\n",
    "convnet.add(Flatten())\n",
    "convnet.add(Dense(units=128))\n",
    "convnet.add(Dropout(0.5))\n",
    "convnet.add(Dense(units=10, activation='softmax'))\n",
    "convnet.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "print(\"convnet parameters: {0:,}\".format(convnet.count_params()))\n",
    "convnet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train \n",
    "* Your input data now needs to have a \"channel\" dimension, as the convolutional filter result will be a multi-channel image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tcg3IqTK26XL",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "convnet_history = convnet.fit(x_train[...,np.newaxis], y_train_one_hot, batch_size=500, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the training performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNLigCrz26XO"
   },
   "source": [
    "Take a look at the loss plot. You can also compare it directly with the loss from the fully-connected network above by plotting both into the same figure. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJDMn63L26XO"
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(18, 4), dpi= 80, facecolor='w', edgecolor='k')\n",
    "ax.plot(convnet_history.history['loss'], label='CNN')\n",
    "# Remove the # to plot both loss curves togehter\n",
    "#ax.plot(history.history['loss'], label='FCN')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on the test data\n",
    "Apply the model to the test data and have a look how many cases are classified correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = convnet.predict(x_test[...,np.newaxis])\n",
    "pred_integer_indices = pred.argmax(axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = y_test - pred_integer_indices\n",
    "correctly_classified_examples = np.where(diff == 0)[0].shape[0]\n",
    "num_examples = y_test.shape[0]\n",
    "wrongly_classified_examples = num_examples - correctly_classified_examples\n",
    "print(f'{correctly_classified_examples} ({correctly_classified_examples/num_examples * 100} %) of the examples are classified correctly while {wrongly_classified_examples} ({wrongly_classified_examples/num_examples * 100}%) are classified wrong.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0NTQY5G26XQ"
   },
   "source": [
    "To inspect the prediction performance in more detail the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) is a useful evaluation metric. It displays for every class the amount of examples which are classified as any of the given classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unNbwnck26XQ"
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "pred = convnet.predict(x_test[...,np.newaxis])\n",
    "cm = sklearn.metrics.confusion_matrix(pred.argmax(axis = -1), y_test)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-a8xehI26XS"
   },
   "source": [
    "It's more intuitive to look at it as a heat map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWG4Typ426XT"
   },
   "outputs": [],
   "source": [
    "plt.matshow(cm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yzp2Regj26XV"
   },
   "source": [
    "Side note: Numpy and Matplotlib are two important, central libraries for numeric computing with Python. In addition, there are also more advanced libraries such as Seaborn, which build upon the things introduced above and offer dedicated functions for complex graphics, such as a combined version of the above matrix + heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YGTlvLbD26XX"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "ax = sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the confusion map:\n",
    "* Which classes are easy for the classifier and which are hard?\n",
    "* Which classes get mixed up a lot? Can you think about a reason for that? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are you satisifed with the results of the classifier? Try out different configurations:\n",
    "* Change the number of epochs used during training\n",
    "* Change the batch size.\n",
    "* Add more layers to the model.\n",
    "* Change the number of filters or the kernel_size of the model\n",
    "* ..."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Session_1_CNN_Classification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
