{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session_4_Generative_Adversarial_Nets.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mtwenzel/image-video-understanding/blob/master/Session_4_Generative_Adversarial_Nets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "xjxgLgoFDsWn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Generative Adversarial Networks\n",
        "* Idea: Optimize one network by letting it compete against another: the adversarial network.\n",
        "* The two networks are usually named\n",
        "    * The Generator network -- aims to generate an output that is indistinguishable from the desired output\n",
        "    * The Discriminator network -- tries to tell the fake (Generator-produced) output from the true output\n",
        "* Training: In each iteration, ...\n",
        "    * A number of fake examples is generated;\n",
        "    * A number of real examples is drawn from the train images;\n",
        "    * The discriminator is trained for some iterations;\n",
        "    * The adversarial model is trained (i.e. the generator is free to adjust weights while the discriminator weights are is fixed in some implementations, and the loss is the discrimination loss)"
      ]
    },
    {
      "metadata": {
        "id": "fu9p_BS5DsWq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "DCGAN on MNIST using Keras\n",
        "Code basis: Rowel Atienza; https://github.com/roatienza/Deep-Learning-Experiments\n",
        "Adapted by Markus Wenzel, Fraunhofer MEVIS\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Reshape\n",
        "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
        "from keras.layers import LeakyReLU, Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.optimizers import RMSprop, ADAM\n",
        "\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kcrYVorvDsWu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ElapsedTimer(object):\n",
        "    def __init__(self):\n",
        "        self.start_time = time.time()\n",
        "    def elapsed(self,sec):\n",
        "        if sec < 60:\n",
        "            return str(sec) + \" sec\"\n",
        "        elif sec < (60 * 60):\n",
        "            return str(sec / 60) + \" min\"\n",
        "        else:\n",
        "            return str(sec / (60 * 60)) + \" hr\"\n",
        "    def elapsed_time(self):\n",
        "        print(\"Elapsed: %s \" % self.elapsed(time.time() - self.start_time) )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KSv6djpODsWx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DCGAN(object):\n",
        "    def __init__(self, img_rows=28, img_cols=28, channel=1):\n",
        "\n",
        "        self.img_rows = img_rows\n",
        "        self.img_cols = img_cols\n",
        "        self.channel = channel\n",
        "        self.D = None   # discriminator\n",
        "        self.G = None   # generator\n",
        "        self.AM = None  # adversarial model\n",
        "        self.DM = None  # discriminator model\n",
        "\n",
        "    # (Wâˆ’F+2P)/S+1\n",
        "    def discriminator(self):\n",
        "        if self.D:\n",
        "            return self.D\n",
        "        self.D = Sequential()\n",
        "        depth = 64\n",
        "        dropout = 0.4\n",
        "        # In: 28 x 28 x 1, depth = 1\n",
        "        # Out: 14 x 14 x 1, depth=64\n",
        "        input_shape = (self.img_rows, self.img_cols, self.channel)\n",
        "        self.D.add(Conv2D(depth*1, 5, strides=2, input_shape=input_shape,\\\n",
        "            padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=0.2))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        self.D.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=0.2))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        self.D.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=0.2))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        self.D.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=0.2))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        # Out: 1-dim probability\n",
        "        self.D.add(Flatten())\n",
        "        self.D.add(Dense(1))\n",
        "        self.D.add(Activation('sigmoid'))\n",
        "        print('*** Discriminator ***')\n",
        "        self.D.summary()\n",
        "        return self.D\n",
        "\n",
        "    def generator(self):\n",
        "        if self.G:\n",
        "            return self.G\n",
        "        self.G = Sequential()\n",
        "        dropout = 0.4\n",
        "        depth = 64+64+64+64\n",
        "        dim = 7\n",
        "        # In: 100\n",
        "        # Out: dim x dim x depth\n",
        "        self.G.add(Dense(dim*dim*depth, input_dim=100))\n",
        "        self.G.add(BatchNormalization(momentum=0.9))\n",
        "        self.G.add(Activation('relu'))\n",
        "        self.G.add(Reshape((dim, dim, depth)))\n",
        "        self.G.add(Dropout(dropout))\n",
        "\n",
        "        # In: dim x dim x depth\n",
        "        # Out: 2*dim x 2*dim x depth/2\n",
        "        self.G.add(UpSampling2D())\n",
        "        self.G.add(Conv2DTranspose(int(depth/2), 5, padding='same'))\n",
        "        self.G.add(BatchNormalization(momentum=0.9))\n",
        "        self.G.add(Activation('relu'))\n",
        "\n",
        "        self.G.add(UpSampling2D())\n",
        "        self.G.add(Conv2DTranspose(int(depth/4), 5, padding='same'))\n",
        "        self.G.add(BatchNormalization(momentum=0.9))\n",
        "        self.G.add(Activation('relu'))\n",
        "\n",
        "        self.G.add(Conv2DTranspose(int(depth/8), 5, padding='same'))\n",
        "        self.G.add(BatchNormalization(momentum=0.9))\n",
        "        self.G.add(Activation('relu'))\n",
        "\n",
        "        # Out: 28 x 28 x 1 grayscale image [0.0,1.0] per pix\n",
        "        self.G.add(Conv2DTranspose(1, 5, padding='same'))\n",
        "        self.G.add(Activation('sigmoid'))\n",
        "        print('*** Generator ***')\n",
        "        self.G.summary()\n",
        "        return self.G\n",
        "\n",
        "    def discriminator_model(self):\n",
        "        if self.DM:\n",
        "            return self.DM\n",
        "        optimizer = RMSprop(lr=0.0002, decay=6e-8)\n",
        "        self.DM = Sequential()\n",
        "        self.DM.add(self.discriminator())\n",
        "        self.DM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n",
        "            metrics=['accuracy'])\n",
        "        return self.DM\n",
        "\n",
        "    def adversarial_model(self):\n",
        "        if self.AM:\n",
        "            return self.AM\n",
        "        optimizer = RMSprop(lr=0.0001, decay=3e-8)\n",
        "        self.AM = Sequential()\n",
        "        self.AM.add(self.generator())\n",
        "        self.AM.add(self.discriminator())\n",
        "        self.AM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n",
        "            metrics=['accuracy'])\n",
        "        return self.AM\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YvvNnz-eDsWz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MNIST_DCGAN(object):\n",
        "    def __init__(self):\n",
        "        self.img_rows = 28\n",
        "        self.img_cols = 28\n",
        "        self.channel = 1\n",
        "\n",
        "        self.x_train = self.read_mnist_keras()\n",
        "\n",
        "        self.DCGAN = DCGAN()\n",
        "        self.discriminator =  self.DCGAN.discriminator_model()\n",
        "        self.adversarial = self.DCGAN.adversarial_model()\n",
        "        self.generator = self.DCGAN.generator()\n",
        "\n",
        "\n",
        "    def read_mnist_keras(self):\n",
        "        from keras.datasets import mnist\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "        return (x_train[5000:] / 255.).reshape(-1, self.img_rows, self.img_cols, 1).astype(np.float32)\n",
        "\n",
        "\n",
        "    def train(self, train_steps=2000, batch_size=256, save_interval=0):\n",
        "        noise_input = None\n",
        "        if save_interval>0:\n",
        "            noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
        "        for i in range(train_steps):\n",
        "            images_train = self.x_train[np.random.randint(0,\n",
        "                self.x_train.shape[0], size=batch_size), :, :, :]\n",
        "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
        "            images_fake = self.generator.predict(noise)\n",
        "            x = np.concatenate((images_train, images_fake))\n",
        "            y = np.ones([2*batch_size, 1])\n",
        "            y[batch_size:, :] = 0\n",
        "            d_loss = self.discriminator.train_on_batch(x, y)\n",
        "\n",
        "            # [mwenzel] note that the discriminator loss is not fixed -- both the generator and the discriminator are optimized!\n",
        "            y = np.ones([batch_size, 1])\n",
        "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
        "            a_loss = self.adversarial.train_on_batch(noise, y)\n",
        "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
        "            log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n",
        "            print(log_mesg)\n",
        "            if save_interval>0:\n",
        "                if (i+1)%save_interval==0:\n",
        "                    self.plot_images(save2file=True, samples=noise_input.shape[0],\\\n",
        "                        noise=noise_input, step=(i+1))\n",
        "\n",
        "    def plot_images(self, save2file=False, fake=True, samples=16, noise=None, step=0):\n",
        "        filename = 'mnist.png'\n",
        "        if fake:\n",
        "            if noise is None:\n",
        "                noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n",
        "            else:\n",
        "                filename = \"mnist_%d.png\" % step\n",
        "            images = self.generator.predict(noise)\n",
        "        else:\n",
        "            i = np.random.randint(0, self.x_train.shape[0], samples)\n",
        "            images = self.x_train[i, :, :, :]\n",
        "\n",
        "        plt.figure(figsize=(10,10))\n",
        "        for i in range(images.shape[0]):\n",
        "            plt.subplot(4, 4, i+1)\n",
        "            image = images[i, :, :, 0]\n",
        "            plt.imshow(image, cmap='gray')\n",
        "            plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        if save2file:\n",
        "            plt.savefig(filename)\n",
        "            plt.close('all')\n",
        "        else:\n",
        "            plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "nH4eU5RJDsW2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    mnist_dcgan = MNIST_DCGAN()\n",
        "    timer = ElapsedTimer()\n",
        "    mnist_dcgan.train(train_steps=1000, batch_size=256, save_interval=100)\n",
        "    timer.elapsed_time()\n",
        "    mnist_dcgan.plot_images(fake=True)\n",
        "    mnist_dcgan.plot_images(fake=False, save2file=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KuB9Mpw3DsW5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Assignment 1: Adapt to larger images.\n",
        "Extend the DCGAN class so that the generator can generate images of arbitrary size. \n",
        "* For this, calculate the required `dims`.\n",
        "* Add the parameter to the class definition.\n",
        "* Create a new class liver_DCGAN to instantiate the new DCGAN appropriately, and to load the image data."
      ]
    },
    {
      "metadata": {
        "id": "4_BuSGyZDsW6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Solution proposal\n",
        "\n",
        "class DCGAN_new(object):\n",
        "    def __init__(self, img_rows=28, img_cols=28, channel=1):\n",
        "\n",
        "        self.img_rows = img_rows\n",
        "        self.img_cols = img_cols\n",
        "        assert self.img_rows == self.img_cols, \"Input image has to be square, at least for now...\"\n",
        "        self.dims     = int(img_rows / 4)\n",
        "        assert self.dims == img_rows / 4, \"Input image extents have to be divisible by 4.\"\n",
        "        self.channel = channel\n",
        "        self.D = None   # discriminator\n",
        "        self.G = None   # generator\n",
        "        self.AM = None  # adversarial model\n",
        "        self.DM = None  # discriminator model\n",
        "\n",
        "    # (Wâˆ’F+2P)/S+1\n",
        "    def discriminator(self):\n",
        "        if self.D:\n",
        "            return self.D\n",
        "        self.D = Sequential()\n",
        "        depth = 64\n",
        "        dropout = 0.2\n",
        "        # In: 28 x 28 x 1, depth = 1\n",
        "        # Out: 14 x 14 x 1, depth=64\n",
        "        input_shape = (self.img_rows, self.img_cols, self.channel)\n",
        "        self.D.add(Conv2D(depth*1, 5, strides=2, input_shape=input_shape,\\\n",
        "            padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=0.2))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        self.D.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=0.2))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        self.D.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=0.2))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        self.D.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=0.2))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        # Out: 1-dim probability\n",
        "        self.D.add(Flatten())\n",
        "        self.D.add(Dense(1))\n",
        "        self.D.add(Activation('sigmoid'))\n",
        "        self.D.summary()\n",
        "        return self.D\n",
        "\n",
        "    def generator(self):\n",
        "        if self.G:\n",
        "            return self.G\n",
        "        self.G = Sequential()\n",
        "        dropout = 0.2\n",
        "        depth = 64+64+64+64\n",
        "        # In: 100\n",
        "        # Out: dims x dims x depth\n",
        "        self.G.add(Dense(self.dims*self.dims*depth, input_dim=100))\n",
        "        self.G.add(BatchNormalization(momentum=0.9))\n",
        "        self.G.add(Activation('relu'))\n",
        "        self.G.add(Reshape((self.dims, self.dims, depth)))\n",
        "        self.G.add(Dropout(dropout))\n",
        "\n",
        "        # In: dim x dim x depth\n",
        "        # Out: 2*dim x 2*dim x depth/2\n",
        "        self.G.add(UpSampling2D())\n",
        "        self.G.add(Conv2DTranspose(int(depth/2), 5, padding='same'))\n",
        "        self.G.add(BatchNormalization(momentum=0.9))\n",
        "        self.G.add(Activation('relu'))\n",
        "\n",
        "        self.G.add(UpSampling2D())\n",
        "        self.G.add(Conv2DTranspose(int(depth/4), 5, padding='same'))\n",
        "        self.G.add(BatchNormalization(momentum=0.9))\n",
        "        self.G.add(Activation('relu'))\n",
        "\n",
        "        self.G.add(Conv2DTranspose(int(depth/8), 5, padding='same'))\n",
        "        self.G.add(BatchNormalization(momentum=0.9))\n",
        "        self.G.add(Activation('relu'))\n",
        "\n",
        "        # Out: 28 x 28 x 1 grayscale image [0.0,1.0] per pix\n",
        "        self.G.add(Conv2DTranspose(1, 5, padding='same'))\n",
        "        self.G.add(Activation('sigmoid'))\n",
        "        self.G.summary()\n",
        "        return self.G\n",
        "\n",
        "    def discriminator_model(self):\n",
        "        if self.DM:\n",
        "            return self.DM\n",
        "        optimizer = RMSprop(lr=0.0001, decay=6e-8)\n",
        "        self.DM = Sequential()\n",
        "        self.DM.add(self.discriminator())\n",
        "        self.DM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n",
        "            metrics=['accuracy'])\n",
        "        return self.DM\n",
        "\n",
        "    def adversarial_model(self):\n",
        "        if self.AM:\n",
        "            return self.AM\n",
        "        optimizer = RMSprop(lr=0.00005, decay=3e-8)\n",
        "        self.AM = Sequential()\n",
        "        self.AM.add(self.generator())\n",
        "        self.AM.add(self.discriminator())\n",
        "        self.AM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n",
        "            metrics=['accuracy'])\n",
        "        return self.AM\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CPFRpqczH69P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash \n",
        "test -e tmp_slices.npz || curl -L \"https://drive.google.com/uc?export=download&id=1R2-H0dhhrj6XNK7Q-MazIWGeFDOf6Zya\" --output tmp_slices.npz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ReNYpE-RDsW9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Solution proposal\n",
        "\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class liver_DCGAN(object):\n",
        "    def __init__(self):\n",
        "        self.img_rows = 76\n",
        "        self.img_cols = 76\n",
        "        self.channel = 1\n",
        "        \n",
        "        TRAINING_SLICE_COUNT = 3000 \n",
        "\n",
        "        loaded = np.load('tmp_slices.npz')\n",
        "\n",
        "        self.x_train = loaded['x_train'][:TRAINING_SLICE_COUNT]\n",
        "        self.y_train = loaded['y_train'][:TRAINING_SLICE_COUNT]\n",
        "\n",
        "        self.x_test = loaded['x_train'][TRAINING_SLICE_COUNT:]\n",
        "        self.y_test = loaded['y_train'][TRAINING_SLICE_COUNT:]\n",
        "\n",
        "        assert len(self.x_train) == len(self.y_train)\n",
        "\n",
        "        self.DCGAN = DCGAN_new(self.img_rows, self.img_cols)\n",
        "        self.discriminator =  self.DCGAN.discriminator_model()\n",
        "        self.adversarial = self.DCGAN.adversarial_model()\n",
        "        self.generator = self.DCGAN.generator()\n",
        "\n",
        "    def train(self, train_steps=2000, batch_size=256, save_interval=0):\n",
        "        noise_input = None\n",
        "        if save_interval>0:\n",
        "            noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
        "        for i in range(train_steps):\n",
        "            images_train = self.x_train[np.random.randint(0,\n",
        "                self.x_train.shape[0], size=batch_size), :, :]\n",
        "            images_train = np.reshape(images_train, images_train.shape + (1,))\n",
        "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
        "            images_fake = self.generator.predict(noise)\n",
        "            x = np.concatenate((images_train, images_fake))\n",
        "            y = np.ones([2*batch_size, 1])\n",
        "            y[batch_size:, :] = 0\n",
        "            d_loss = self.discriminator.train_on_batch(x, y)\n",
        "\n",
        "            y = np.ones([batch_size, 1])\n",
        "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
        "            a_loss = self.adversarial.train_on_batch(noise, y)\n",
        "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
        "            log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n",
        "            print(log_mesg)\n",
        "            if save_interval>0:\n",
        "                if (i+1)%save_interval==0:\n",
        "                    self.plot_images(save2file=True, samples=noise_input.shape[0],\\\n",
        "                        noise=noise_input, step=(i+1))\n",
        "\n",
        "    def plot_images(self, save2file=False, fake=True, samples=16, noise=None, step=0):\n",
        "        filename = 'liver.png'\n",
        "        if fake:\n",
        "            if noise is None:\n",
        "                noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n",
        "            else:\n",
        "                filename = \"liver_%d.png\" % step\n",
        "            images = self.generator.predict(noise)\n",
        "        else:\n",
        "            i = np.random.randint(0, self.x_train.shape[0], samples)\n",
        "            images = self.x_train[i, :, :, :]\n",
        "\n",
        "        plt.figure(figsize=(10,10))\n",
        "        for i in range(images.shape[0]):\n",
        "            plt.subplot(4, 4, i+1)\n",
        "            image = images[i, :, :, :]\n",
        "            image = np.reshape(image, [self.img_rows, self.img_cols])\n",
        "            plt.imshow(image, cmap='gray')\n",
        "            plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        if save2file:\n",
        "            plt.savefig(filename)\n",
        "            plt.close('all')\n",
        "        else:\n",
        "            plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "Cx8YwXSGDsW_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    liver_dcgan = liver_DCGAN()\n",
        "    timer = ElapsedTimer()\n",
        "    liver_dcgan.train(train_steps=10000, batch_size=256, save_interval=50)\n",
        "    timer.elapsed_time()\n",
        "    liver_dcgan.plot_images(fake=True)\n",
        "    liver_dcgan.plot_images(fake=False, save2file=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ujx0VAAODsXC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Problems"
      ]
    },
    {
      "metadata": {
        "id": "wkLMoJ3UDsXD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* If you observe that the discriminator loss drops rapidly but the adversarial loss doesn't: why is this a problem? What is a solution?\n",
        "\n",
        "[` `] Relatively lower the learning rate of the discriminator.\n",
        "\n",
        "[` `] Relatively lower the learning rate of the generator.\n",
        "\n",
        "[` `] Increase the learning rates of both generator and discriminator."
      ]
    },
    {
      "metadata": {
        "id": "W6VTWpo5DsXE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* What is the effect of \n",
        "  * (not) using dropout, \n",
        "  * of using it only in generator or discriminator, \n",
        "  * or using too high (> 0.6) values? \n",
        "  \n",
        "Run experiments. (Attention: Run at least 500-1000 iterations before effects become apparent.)"
      ]
    },
    {
      "metadata": {
        "id": "YUDEfPWrDsXF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Assignment 2: Semantic Segmentation using Adversarial Training\n",
        "Change the DCGAN so that it produces segmentations instead of fake originals.\n",
        "* For this, replace the generator with a segmentation network.\n",
        "* The adversarial net receives the original and generated segmentations.\n",
        "\n",
        "Read \"Semantic Segmentation using Adversarial Networks\" (Luc et al. 2016). "
      ]
    },
    {
      "metadata": {
        "id": "IaJFAKitDsXG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Solution proposal\n",
        "from keras.layers import UpSampling2D, MaxPool2D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
        "from keras.layers import Input, concatenate\n",
        "from keras.models import Model, Sequential\n",
        "import keras.backend as K\n",
        "\n",
        "\n",
        "def dice_coef(y_true, y_pred, smooth=1):\n",
        "    \"\"\"\n",
        "    Dice = (2*|X & Y|)/ (|X|+ |Y|)\n",
        "         =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))\n",
        "    ref: https://arxiv.org/pdf/1606.04797v1.pdf\n",
        "    @url: https://gist.github.com/wassname/7793e2058c5c9dacb5212c0ac0b18a8a\n",
        "    @author: wassname\n",
        "    \"\"\"\n",
        "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
        "    return (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return 1 - dice_coef(y_true, y_pred)\n",
        "\n",
        "\n",
        "\n",
        "class DCGAN_segmentation(object):\n",
        "    def __init__(self, img_rows=28, img_cols=28, channel=1):\n",
        "\n",
        "        self.img_rows = img_rows\n",
        "        self.img_cols = img_cols\n",
        "        assert self.img_rows == self.img_cols, \"Input image has to be square, at least for now...\"\n",
        "        self.dims     = int(img_rows / 4)\n",
        "        assert self.dims == img_rows / 4, \"Input image extents have to be divisible by 4.\"\n",
        "        self.channel = channel\n",
        "        self.D = None   # discriminator\n",
        "        self.G = None   # generator\n",
        "        self.AM = None  # adversarial model\n",
        "        self.DM = None  # discriminator model\n",
        "        print(\"DCGAN_segmentation initialised.\")\n",
        "\n",
        "    def conv_bn_functional(self, model, filters=32, kernel_size=(3,3), batch_norm=True, activation='prelu', padding='same', kernel_regularizer=None):\n",
        "        if batch_norm:\n",
        "            model = BatchNormalization()(model)\n",
        "        if activation == 'prelu':\n",
        "            model = Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation='linear', kernel_regularizer=kernel_regularizer)(model)\n",
        "            model = PReLU()(model)\n",
        "        elif activation == 'lrelu':\n",
        "            model = Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation='linear', kernel_regularizer=kernel_regularizer)(model)\n",
        "            model = LeakyReLU()(model)\n",
        "        else:\n",
        "            model = Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation=activation, kernel_regularizer=kernel_regularizer)(model)\n",
        "        return model\n",
        "\n",
        "    def generator_functional(self, _filters=32, _filters_add=0, _kernel_size=(3,3), _padding='same', _activation='prelu', _kernel_regularizer=None, _final_layer_nonlinearity='sigmoid', _batch_norm=True):\n",
        "        if self.G:\n",
        "            return self.G\n",
        "\n",
        "        input_layer = Input(shape=(self.img_rows,self.img_cols,1))\n",
        "\n",
        "        x0 = self.conv_bn_functional(input_layer, filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
        "        x0 = self.conv_bn_functional(x0,          filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
        "        x1 = MaxPool2D()(x0)\n",
        "\n",
        "        x1 = self.conv_bn_functional(x1,          filters=_filters+_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
        "        x1 = self.conv_bn_functional(x1,          filters=_filters+_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
        "        x2 = MaxPool2D()(x1)\n",
        "\n",
        "        x2 = self.conv_bn_functional(x2,          filters=_filters+2*_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
        "        x2 = self.conv_bn_functional(x2,          filters=_filters+2*_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
        "        x3 = UpSampling2D()(x2)\n",
        "\n",
        "        x3 = concatenate([x1,x3])\n",
        "        x3 = self.conv_bn_functional(x3,          filters=_filters+_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
        "        x3 = self.conv_bn_functional(x3,          filters=_filters+_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
        "        x4 = UpSampling2D()(x3)\n",
        "\n",
        "        x4 = concatenate([x0,x4])\n",
        "        x4 = self.conv_bn_functional(x4,          filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
        "        x4 = self.conv_bn_functional(x4,          filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
        "\n",
        "        output_layer = Conv2D(1, kernel_size=(1,1), activation=_final_layer_nonlinearity)(x4)\n",
        "\n",
        "        self.G = Model(input_layer, output_layer)\n",
        "        self.G.input_layer = input_layer\n",
        "        self.G.output_layer = output_layer\n",
        "        #self.G.summary()\n",
        "        return self.G    \n",
        "    \n",
        "    def discriminator(self):\n",
        "        if self.D:\n",
        "            return self.D\n",
        "        self.D = Sequential()\n",
        "        depth = 64\n",
        "        dropout = 0.2\n",
        "        input_shape = (self.img_rows, self.img_cols, self.channel)\n",
        "        self.D.add(Conv2D(depth*1, 5, strides=2, input_shape=input_shape,\\\n",
        "            padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=0.2))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        self.D.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=0.2))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        self.D.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=0.2))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        self.D.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=0.2))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        # Out: 1-dim probability\n",
        "        self.D.add(Flatten())\n",
        "        self.D.add(Dense(1))\n",
        "        self.D.add(Activation('sigmoid'))\n",
        "        #self.D.summary()\n",
        "        return self.D\n",
        "\n",
        "    # The learning rate of discriminator and adversarial model need to be adjusted carefully.\n",
        "    def discriminator_model(self):\n",
        "        if self.DM:\n",
        "            return self.DM\n",
        "        optimizer = RMSprop(lr=0.00001, decay=6e-8)\n",
        "        self.DM = Sequential()\n",
        "        self.DM.add(self.discriminator())\n",
        "        self.DM.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "        return self.DM\n",
        "    \n",
        "    def adversarial_model(self):\n",
        "        if self.AM:\n",
        "            return self.AM\n",
        "        optimizer = RMSprop(lr=0.00001, decay=3e-8)\n",
        "        #optimizer = 'adadelta'\n",
        "\n",
        "        discriminator_functional = self.discriminator_model().model\n",
        "        self.AM = discriminator_functional(self.generator_functional(_activation='relu', _batch_norm=True).get_output_at(0))\n",
        "        self.AM = Model(inputs=self.G.input_layer, outputs=[self.G.output_layer, self.AM])\n",
        "        self.AM.compile(loss=[dice_coef_loss, 'binary_crossentropy'], loss_weights=[0.6, 0.4], optimizer=optimizer, metrics=['accuracy'])\n",
        "        return self.AM\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OXBKJht4DsXI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Solution proposal\n",
        "\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class segmentation_DCGAN(object):\n",
        "    def __init__(self):\n",
        "        self.img_rows = 76\n",
        "        self.img_cols = 76\n",
        "        self.channel = 1\n",
        "        \n",
        "        self.load_data()\n",
        "\n",
        "        self.DCGAN = DCGAN_segmentation(self.img_rows, self.img_cols)\n",
        "        self.discriminator =  self.DCGAN.discriminator_model()\n",
        "        self.adversarial = self.DCGAN.adversarial_model()\n",
        "        self.generator = self.DCGAN.generator_functional(_activation='relu', _batch_norm=True)\n",
        "\n",
        "    def load_data(self):\n",
        "        TRAINING_SLICE_COUNT = 3000 \n",
        "\n",
        "        loaded = np.load('tmp_slices.npz')\n",
        "\n",
        "        self.x_train = loaded['x_train'][:TRAINING_SLICE_COUNT]\n",
        "        self.y_train = loaded['y_train'][:TRAINING_SLICE_COUNT]\n",
        "\n",
        "        self.x_test = loaded['x_train'][TRAINING_SLICE_COUNT:]\n",
        "        self.y_test = loaded['y_train'][TRAINING_SLICE_COUNT:]\n",
        "\n",
        "        assert len(self.x_train) == len(self.y_train)\n",
        "        \n",
        "        \n",
        "    def train(self, train_steps=2000, batch_size=256, save_interval=0, save2file=True, prefix='segmentation'):\n",
        "        if save_interval>0:\n",
        "            # Select a number of test images to predict\n",
        "            rand_selection = np.random.randint(0, self.y_train.shape[0], size=16)\n",
        "            originals_input = self.x_train[rand_selection, :, :]\n",
        "            originals_input = np.reshape(originals_input, originals_input.shape + (1,))\n",
        "        for i in range(train_steps):\n",
        "            # images_train becomes the masks.\n",
        "            # noise (the generator input) becomes the originals.\n",
        "            # images_fake are the generator results -- the predicted masks.\n",
        "            \n",
        "            # (I) Train discriminator\n",
        "            rand_selection = np.random.randint(0, self.x_train.shape[0], size=batch_size)\n",
        "            originals_train = self.x_train[rand_selection, :, :]\n",
        "            originals_train = np.reshape(originals_train, originals_train.shape + (1,))\n",
        "            masks_train = self.y_train[rand_selection, :, :, np.newaxis]\n",
        "            masks_fake = self.generator.predict(originals_train)\n",
        "            x = np.concatenate((masks_train, masks_fake))\n",
        "            y = np.ones([2*batch_size, 1])\n",
        "            y[batch_size:, :] = 0\n",
        "            d_loss = self.discriminator.train_on_batch(x, y)\n",
        "            \n",
        "            # (III) Train adversarial\n",
        "            rand_selection = np.random.randint(0, self.y_train.shape[0], size=batch_size)\n",
        "            y = np.ones([batch_size, 1])\n",
        "            originals_train = self.x_train[rand_selection, :, :, np.newaxis]\n",
        "            a_loss = self.adversarial.train_on_batch(originals_train, [masks_train, y])\n",
        "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
        "            log_mesg = \"%s  [A loss: %f/%f/%f, acc: %f/%f]\" % (log_mesg, a_loss[0], a_loss[1], a_loss[2], a_loss[3], a_loss[4])\n",
        "            print(log_mesg)\n",
        "            if save_interval>0:\n",
        "                if (i+1)%save_interval==0:\n",
        "                    self.plot_images(save2file=save2file, samples=originals_input.shape[0],\\\n",
        "                        noise=originals_input, step=(i+1), prefix=prefix)\n",
        "\n",
        "    def plot_images(self, save2file=False, fake=True, samples=16, noise=None, step=0, prefix='segmentation'):\n",
        "        filename = '%s.png' % prefix\n",
        "        if fake:\n",
        "            if noise is None:\n",
        "                # Replace noise with random input images\n",
        "                rand_selection = np.random.randint(0, self.x_train.shape[0], size=samples)\n",
        "                originals_train = self.x_train[rand_selection, :, :]\n",
        "                noise = np.reshape(originals_train, originals_train.shape + (1,))\n",
        "            else:\n",
        "                filename = \"%s_%d.png\" % (prefix, step)\n",
        "            images = self.generator.predict(noise)\n",
        "        else:\n",
        "            i = np.random.randint(0, self.x_train.shape[0], samples)\n",
        "            images = self.x_train[i, :, :, :]\n",
        "            images = np.reshape(images, images.shape + (1,))\n",
        "\n",
        "        plt.figure(figsize=(10,10))\n",
        "        for i in range(images.shape[0]):\n",
        "            plt.subplot(4, 4, i+1)\n",
        "            image = images[i, :, :, :]\n",
        "            image = np.reshape(image, [self.img_rows, self.img_cols])\n",
        "            plt.imshow(image, cmap='gray')\n",
        "            plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        if save2file:\n",
        "            plt.savefig(filename)\n",
        "            plt.close('all')\n",
        "        else:\n",
        "            plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "306Tc-B8DsXL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "liver_dcgan = segmentation_DCGAN()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "5bWisq_kDsXN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "timer = ElapsedTimer()\n",
        "# Look at random batches of 200 slices each. \n",
        "liver_dcgan.train(train_steps=10, batch_size=200, save_interval=100, save2file=True, prefix='segmentation_60-40-rmsprop')\n",
        "timer.elapsed_time()\n",
        "liver_dcgan.plot_images(fake=True)\n",
        "#liver_dcgan.plot_images(fake=False, save2file=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LSsrn5oeDsXP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}